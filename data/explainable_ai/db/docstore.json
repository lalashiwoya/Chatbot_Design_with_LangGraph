{"docstore/metadata": {"https://www.ibm.com/topics/explainable-ai": {"doc_hash": "d1ff78a3bd8dbb6f44766135e11120a63a9f3a736deae13896daf7917279ea0f"}, "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/": {"doc_hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d"}, "9f57d295-fafe-426e-822a-9cbba1f4a8f4": {"doc_hash": "c5bf74327d8b18622d1c06fe4e3012a8d0d185fc5a547839d72cdce87794c58c", "ref_doc_id": "https://www.ibm.com/topics/explainable-ai"}, "1feb8645-6694-4722-af86-ba2e660a6849": {"doc_hash": "234fe64b2aa650251455285b95de6b14b5936b00c9076de4d59385671833517e", "ref_doc_id": "https://www.ibm.com/topics/explainable-ai"}, "97a095d0-e0e0-46df-ba29-74126e965f18": {"doc_hash": "c9e437d90e435c4c699d65f01f543162cc9dfe772ee43f69a874c8d2b4a2beff", "ref_doc_id": "https://www.ibm.com/topics/explainable-ai"}, "e1c7088a-a448-4e66-8926-a7baa3ad9c15": {"doc_hash": "ef8b8db76fe446659cb3c0ddd867bcef0e29c8d41dc1fad43467358cf81a8f1e", "ref_doc_id": "https://www.ibm.com/topics/explainable-ai"}, "e8d3c295-9075-462c-a203-d0a85d3fdc41": {"doc_hash": "9480fd94890d568948e0036a17ac25c128d32042ff1292512622099770cce17b", "ref_doc_id": "https://www.ibm.com/topics/explainable-ai"}, "16566743-ce54-4b5c-a21e-7de497a8b149": {"doc_hash": "bf4821efb135ddf1e98be9152f4f086bba72054f1ab2f9f92fcc9c64a4dda1dd", "ref_doc_id": "https://www.ibm.com/topics/explainable-ai"}, "d268e45d-6d58-4c89-9a44-3caf786a32ac": {"doc_hash": "591cbb770a5a0345fc015375470fe359ed5c993f337fd9d0855c698efde2f4bd", "ref_doc_id": "https://www.ibm.com/topics/explainable-ai"}, "53d12853-52b6-4f0d-8d91-f567570d714f": {"doc_hash": "a68fbb3c3f0605c3a5f335abbf514baeb27b7fbe5c8d69528eb023a1238d0225", "ref_doc_id": "https://www.ibm.com/topics/explainable-ai"}, "106e0837-b60e-4d6e-b2ca-c28711221357": {"doc_hash": "6fdd68d34655f7edae0e92a3b18b568845893f3781278516c4790f933d3b11f8", "ref_doc_id": "https://www.ibm.com/topics/explainable-ai"}, "6b21addf-c14e-4402-9d91-b0101720a55a": {"doc_hash": "8110f2739ee11bc7eeda3f9f3c788d9a567e3fa02bd8a223a3387d31fff6346a", "ref_doc_id": "https://www.ibm.com/topics/explainable-ai"}, "a00dfea6-58cf-45a6-8ffa-169bf7b0de66": {"doc_hash": "ce1379863d9717c3f2806ecc034c6e2ed12bf4a9d4397e72d7ad07a9468a3888", "ref_doc_id": "https://www.ibm.com/topics/explainable-ai"}, "5251c649-75c1-4432-a865-69c8bfb128df": {"doc_hash": "919b4b93829953ed789fe0293b5d18a84ab16c86b1f3534d6891eef2efbf4a51", "ref_doc_id": "https://www.ibm.com/topics/explainable-ai"}, "a8b4220e-93e9-4cdf-be93-61bf715c1bb2": {"doc_hash": "fbcb03b3fcb7df6b22436e236abefad451ecb9a507af5ebd65d8c0d5e6ebb2d1", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "7d1a0380-9a34-429f-ab14-a96cae64e46a": {"doc_hash": "edaf3a4afef56df78da99a2b170cfc69340ca4ae8dd364b75929cebb428028d4", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "b36500f2-bc6a-46dd-98fd-cf2ccb82fa0e": {"doc_hash": "c4e6550df8503d1951bfaf6ff7bdb28eb757b60d16b1c821eacba7ffc6298d57", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "da841b1d-3071-42f9-a797-296637986953": {"doc_hash": "affd8028be3f3aa1475e4c4c043de7e2ddbd9cec552f48989f51504158b36e40", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "5671e77b-3659-4835-b9dc-d009dd7a2b6a": {"doc_hash": "b06c5fdaee6b469de0decc57a6928110f4371cc0e875e3afbf9dc84045a1a370", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "9df11f5c-a6cf-4635-a60e-084639ecdb3d": {"doc_hash": "1136e643b83c593805b33b71159ac61b23840d694d0fc00a2dbe4655cc13c246", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "89562f45-aab8-4a3c-9cfc-e2000596ed19": {"doc_hash": "dec7fadc95c13c449fefcb172a4ed839f8b2696e9f524f189d5b26fe1cd449f2", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "039da86e-f098-4183-8bb1-fe348e2cf70c": {"doc_hash": "dd4bc167e83877d13916a6d3db57928b123e2bc38cef1f0af8607335c6e3b63a", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "8badb81f-4864-4363-9e34-a532cdef91be": {"doc_hash": "0e5ee061d2fe468071707f4685fd92082ffe7115bd05de227dae1e7fdb5c6f44", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "d267af47-ac77-4ef0-8738-a1e895a247ed": {"doc_hash": "eef7d21e703ac3d63f593fd81ec6d7b74f1e804cd0240016e46ff756722a7ef0", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "716c5086-4dbd-4527-8071-2e9569c61009": {"doc_hash": "801a6e929c93643e500c94a7c9a11da2aa8edd3c60bea0ffa9c7811a3525b6fc", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "53f9af88-1698-46ef-97f2-da2a48d83d65": {"doc_hash": "ac1ceaa184dc1453028df7f6940f1efe86392ba259e529f82508dea1400292e8", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "adbc24d0-bcb8-4ed2-a1ea-c427579a3f4b": {"doc_hash": "97ccfcbbf00e7a816c021375d55c3ea75fbf8cb521cf7b0109fec65d4bfa42a4", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "b0a7836d-dde3-4642-a16b-debbcada7d2b": {"doc_hash": "c46dc99e99cbf8e8aa3adc75252f51faaed82c68a01af0552ab6dd248e47b3a8", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "0dd3718f-1feb-4932-bbdc-a6199eede44f": {"doc_hash": "233d97f22339a029ec3cf6a91b1cfd252ad6ed6eff40752d17d95bb3d4c218b9", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "569c0528-8d77-4eaa-932b-ff1adcc454ba": {"doc_hash": "aad34bf32c966bec6b1264aa72910c3810574637b3247b94f41436cc618bd677", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "a3be4d10-8f0e-429c-8132-c1317fc37533": {"doc_hash": "4f25ac7f2bed67a5804b0f34724cdcd08a0fcbfaefc0af74bd8212e14dbf6b29", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "39845546-ba5c-42aa-a3dc-e834b5cbfebf": {"doc_hash": "d40834cc928bf3727d568ce2d9c0a8803aa1ea02609c9a48cbded0880d1c3d65", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "8aa20507-e00a-43f3-9866-f51390318b2d": {"doc_hash": "6e963646aacc8aebadcc4b26027af687254c45aa6294f7ef8b34f1cd66c15b60", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "621495eb-3106-4a69-96d9-16c3196a2bcf": {"doc_hash": "2a2af55325e03b7eb93a74c8b6b7197fcc2f5381a3143147f966e3a6a16f1734", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "31a351dc-d940-422b-95b9-a5e852498d8e": {"doc_hash": "3600f0700d4a86a06eae06ac3d330b3fb773ba44b7d7599b33063fc4a356621d", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "12b34e20-4656-4930-8506-682c76a43aac": {"doc_hash": "61da7dc94042f5d33ac50d39e7c62903145f92449fd3662dad452c141c573482", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "18c8fc81-6253-4073-b831-415c6c5c4c62": {"doc_hash": "26391e6f1546932297be5b3bae5d781e89044859c2b2c0d5b47d184ba6d20110", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "1a4d654c-ec3b-446a-b6a0-8ca0b7fec326": {"doc_hash": "6ef691a048d0df033701ffceada44bb33ef40c47192c35e47b370108669b3dc6", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "28c504b2-2097-4de0-9d83-5ce114f485d2": {"doc_hash": "4c1e50d997f5da9cdd2d5edc627979a951e422e06e26af0738b891e9750a7278", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "e003936c-f444-4aed-b2a0-dcb1690975d9": {"doc_hash": "d88b34a65b2c01cfb253708710c20dae739e97b9573b6beae570b5d91cd2cffe", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "cb9ab836-8cd3-429a-82cf-c28a2a860115": {"doc_hash": "55d1e3d7b373a99bf74b5a5a3473c0770863ecd05b8d42c3f3807d79bc87bad4", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "1e983d86-b462-4252-acb4-df7b4c093305": {"doc_hash": "5a91f1e44192067a9a91d81f9eaf0710f024c519590ed862027590fda97b69ee", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "28b56442-dbe6-431d-a41f-efe33a34606c": {"doc_hash": "801a6e929c93643e500c94a7c9a11da2aa8edd3c60bea0ffa9c7811a3525b6fc", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "61e29ec7-a1ae-407c-a199-2d4c8f45dd2c": {"doc_hash": "87ad5147af52c1093e8acdd4d8b3c719b92e360dccb93251616e623a32b87480", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "9c50295b-6059-41ab-a501-534ccabbe808": {"doc_hash": "e1da1cbb16f4534b653f4a2a434014e4a4bb5e7a3b74f8eb806dd0876b36e86d", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "055aa26d-7ce4-4922-932a-6d2f9b897d65": {"doc_hash": "f9e97353df78ceb3ba92b1c1a84eb64e7d7c565eed77a9ce4a41566ab4aa3edd", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "9148de6a-dc0b-42e7-ac6a-9c2d93eb401b": {"doc_hash": "0b4678ce4a8e5bfa9550e8c5da2fc0af36019a104ebeb5aa00a0c327a89c5830", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "f54cff7d-d34e-4ba6-ab1b-809a3eef9751": {"doc_hash": "47b98402ce499543aeaaf5e86c3bafc75299e88df6af75ba6d74b0a42105ab52", "ref_doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}}, "docstore/data": {"9f57d295-fafe-426e-822a-9cbba1f4a8f4": {"__data__": {"id_": "9f57d295-fafe-426e-822a-9cbba1f4a8f4", "embedding": null, "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.ibm.com/topics/explainable-ai", "node_type": "4", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "d1ff78a3bd8dbb6f44766135e11120a63a9f3a736deae13896daf7917279ea0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1feb8645-6694-4722-af86-ba2e660a6849", "node_type": "1", "metadata": {}, "hash": "8eabc4058191347c0014992e0c1dc200ecb9aece7c94b80d45157590647c15b8", "class_name": "RelatedNodeInfo"}}, "text": "What is explainable AI?\n\nExplore IBM's explainable AI solution Subscribe to AI Topic Updates\n\n![Illustration with collage of pictograms of clouds, pie chart, graph\npictograms on the following](/content/dam/connectedassets-adobe-cms/worldwide-\ncontent/creative-assets/s-migr/ul/g/64/ea/content-hub-analytic-page-leadspace-\nshort.component.xl.ts=1712905748212.png/content/adobe-\ncms/us/en/topics/explainable-ai/_jcr_content/root/leadspace)\n\nWhat is explainable AI?\n\nExplainable artificial intelligence (XAI) is a set of processes and methods\nthat allows human users to comprehend and trust the results and output created\nby machine learning algorithms.\n\nExplainable [AI](https://www.ibm.com/consulting/artificial-intelligence) is\nused to describe an AI model, its expected impact and potential biases. It\nhelps characterize model accuracy, fairness, transparency and outcomes in AI-\npowered decision making. Explainable AI is crucial for an organization in\nbuilding trust and confidence when putting AI models into production. AI\nexplainability also helps an organization adopt a responsible approach to AI\ndevelopment.\n\nAs AI becomes more advanced, humans are challenged to comprehend and retrace\nhow the algorithm came to a result.", "start_char_idx": 0, "end_char_idx": 1231, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1feb8645-6694-4722-af86-ba2e660a6849": {"__data__": {"id_": "1feb8645-6694-4722-af86-ba2e660a6849", "embedding": null, "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.ibm.com/topics/explainable-ai", "node_type": "4", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "d1ff78a3bd8dbb6f44766135e11120a63a9f3a736deae13896daf7917279ea0f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f57d295-fafe-426e-822a-9cbba1f4a8f4", "node_type": "1", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "c5bf74327d8b18622d1c06fe4e3012a8d0d185fc5a547839d72cdce87794c58c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97a095d0-e0e0-46df-ba29-74126e965f18", "node_type": "1", "metadata": {}, "hash": "98ed3623ab37c94e75d7a2bbf25cdaf3bf16607eabdc4da997370cada12ba635", "class_name": "RelatedNodeInfo"}}, "text": "As AI becomes more advanced, humans are challenged to comprehend and retrace\nhow the algorithm came to a result. The whole calculation process is turned\ninto what is commonly referred to as a \u201cblack box\" that is impossible to\ninterpret. These black box models are created directly from the data. And, not\neven the engineers or data scientists who create the algorithm can understand\nor explain what exactly is happening inside them or how the AI algorithm\narrived at a specific result.\n\nThere are many advantages to understanding how an AI-enabled system has led to\na specific output. Explainability can help developers ensure that the system\nis working as expected, it might be necessary to meet regulatory standards, or\nit might be important in allowing those affected by a decision to challenge or\nchange that outcome.\u00b9\n\nWhite paper  Why AI governance is a business imperative for scaling enterprise\nAI\n\nLearn about barriers to AI adoptions, particularly lack of AI governance and\nrisk management solutions.", "start_char_idx": 1119, "end_char_idx": 2129, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97a095d0-e0e0-46df-ba29-74126e965f18": {"__data__": {"id_": "97a095d0-e0e0-46df-ba29-74126e965f18", "embedding": null, "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.ibm.com/topics/explainable-ai", "node_type": "4", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "d1ff78a3bd8dbb6f44766135e11120a63a9f3a736deae13896daf7917279ea0f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1feb8645-6694-4722-af86-ba2e660a6849", "node_type": "1", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "234fe64b2aa650251455285b95de6b14b5936b00c9076de4d59385671833517e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1c7088a-a448-4e66-8926-a7baa3ad9c15", "node_type": "1", "metadata": {}, "hash": "b72a2da500d0abb1a28c2bbcd6f240ca0759491830ece633cce69a2a893a2114", "class_name": "RelatedNodeInfo"}}, "text": "Related content\n\nRead the guide for data leaders\n\nBegin your journey to AI\n\n[ Learn how to scale AI ](https://www.ibm.com/resources/the-data-\ndifferentiator/scale-ai)\n\n[ Explore the AI Academy ](https://www.ibm.com/think/ai-academy)\n\nWhy explainable AI matters\n\nIt is crucial for an organization to have a full understanding of the AI\ndecision-making processes with model monitoring and accountability of AI and\nnot to trust them blindly. Explainable AI can help humans understand and\nexplain machine learning (ML) algorithms, deep learning and neural networks.\n\nML models are often thought of as black boxes that are impossible to\ninterpret.\u00b2 Neural networks used in deep learning are some of the hardest for\na human to understand. Bias, often based on race, gender, age or location, has\nbeen a long-standing risk in training AI models. Further, AI model performance\ncan drift or degrade because production data differs from training data. This\nmakes it crucial for a business to continuously monitor and manage models to\npromote AI explainability while measuring the business impact of using such\nalgorithms. Explainable AI also helps promote end user trust, model\nauditability and productive use of AI. It also mitigates compliance, legal,\nsecurity and reputational risks of production AI.", "start_char_idx": 2131, "end_char_idx": 3423, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1c7088a-a448-4e66-8926-a7baa3ad9c15": {"__data__": {"id_": "e1c7088a-a448-4e66-8926-a7baa3ad9c15", "embedding": null, "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.ibm.com/topics/explainable-ai", "node_type": "4", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "d1ff78a3bd8dbb6f44766135e11120a63a9f3a736deae13896daf7917279ea0f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97a095d0-e0e0-46df-ba29-74126e965f18", "node_type": "1", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "c9e437d90e435c4c699d65f01f543162cc9dfe772ee43f69a874c8d2b4a2beff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8d3c295-9075-462c-a203-d0a85d3fdc41", "node_type": "1", "metadata": {}, "hash": "5a04ac155bf7f900cb41479a5b2137b055d65e4a405842b60faecdfdd7cf6d35", "class_name": "RelatedNodeInfo"}}, "text": "It also mitigates compliance, legal,\nsecurity and reputational risks of production AI.\n\nExplainable AI is one of the key requirements for implementing responsible AI,\na methodology for the large-scale implementation of AI methods in real\norganizations with fairness, model explainability and accountability.\u00b3 To help\nadopt AI responsibly, organizations need to embed ethical principles into AI\napplications and processes by building AI systems based on trust and\ntransparency.\n\nNow available: watsonx.governance\n\nAccelerate responsible, transparent and explainable workflows for generative\nAI built on third-party platforms\n\nTry watsonx.governance\n\nLearn more about AI ethics\n\nHow explainable AI works\n\nWith explainable AI \u2013 as well as interpretable machine learning \u2013\norganizations can gain access to AI technology\u2019s underlying decision-making\nand are empowered to make adjustments. Explainable AI can improve the user\nexperience of a product or service by helping the end user trust that the AI\nis making good decisions. When do AI systems give enough confidence in the\ndecision that you can trust it, and how can the AI system correct errors that\narise?\u2074\n\nAs AI becomes more advanced, ML processes still need to be understood and\ncontrolled to ensure AI model results are accurate.", "start_char_idx": 3337, "end_char_idx": 4621, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8d3c295-9075-462c-a203-d0a85d3fdc41": {"__data__": {"id_": "e8d3c295-9075-462c-a203-d0a85d3fdc41", "embedding": null, "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.ibm.com/topics/explainable-ai", "node_type": "4", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "d1ff78a3bd8dbb6f44766135e11120a63a9f3a736deae13896daf7917279ea0f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1c7088a-a448-4e66-8926-a7baa3ad9c15", "node_type": "1", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "ef8b8db76fe446659cb3c0ddd867bcef0e29c8d41dc1fad43467358cf81a8f1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16566743-ce54-4b5c-a21e-7de497a8b149", "node_type": "1", "metadata": {}, "hash": "72d56fceca5670ff33ee6e7eeee30cae0d9002a5c46db4c91f66bf7b2731c439", "class_name": "RelatedNodeInfo"}}, "text": "Let\u2019s look at the\ndifference between AI and XAI, the methods and techniques used to turn AI to\nXAI, and the difference between interpreting and explaining AI processes.\n\n**Comparing AI and XAI**  \nWhat exactly is the difference between \u201cregular\u201d AI and explainable AI? XAI\nimplements specific techniques and methods to ensure that each decision made\nduring the ML process can be traced and explained. AI, on the other hand,\noften arrives at a result using an ML algorithm, but the architects of the AI\nsystems do not fully understand how the algorithm reached that result. This\nmakes it hard to check for accuracy and leads to loss of control,\naccountability and auditability.\n\n**Explainable AI techniques  \n**The setup of XAI techniques consists of three main methods. Prediction\naccuracy and traceability address technology requirements while decision\nunderstanding addresses human needs. Explainable AI \u2014 especially explainable\nmachine learning \u2014 will be essential if future warfighters are to understand,\nappropriately trust, and effectively manage an emerging generation of\nartificially intelligent machine partners.\u2075\n\nPrediction accuracy  \nAccuracy is a key component of how successful the use of AI is in everyday\noperation. By running simulations and comparing XAI output to the results in\nthe training data set, the prediction accuracy can be determined.", "start_char_idx": 4622, "end_char_idx": 5985, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16566743-ce54-4b5c-a21e-7de497a8b149": {"__data__": {"id_": "16566743-ce54-4b5c-a21e-7de497a8b149", "embedding": null, "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.ibm.com/topics/explainable-ai", "node_type": "4", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "d1ff78a3bd8dbb6f44766135e11120a63a9f3a736deae13896daf7917279ea0f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8d3c295-9075-462c-a203-d0a85d3fdc41", "node_type": "1", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "9480fd94890d568948e0036a17ac25c128d32042ff1292512622099770cce17b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d268e45d-6d58-4c89-9a44-3caf786a32ac", "node_type": "1", "metadata": {}, "hash": "2523a689da654be3355204296cb1510f01d92ed3e26e55a1f2733164bed6005e", "class_name": "RelatedNodeInfo"}}, "text": "By running simulations and comparing XAI output to the results in\nthe training data set, the prediction accuracy can be determined. The most\npopular technique used for this is Local Interpretable Model-Agnostic\nExplanations (LIME), which explains the prediction of classifiers by the ML\nalgorithm.\n\nTraceability  \nTraceability is another key technique for accomplishing XAI. This is achieved,\nfor example, by limiting the way decisions can be made and setting up a\nnarrower scope for ML rules and features. An example of a traceability XAI\ntechnique is DeepLIFT (Deep Learning Important FeaTures), which compares the\nactivation of each neuron to its reference neuron and shows a traceable link\nbetween each activated neuron and even shows dependencies between them.\n\nDecision understanding  \nThis is the human factor. Many people have a distrust in AI, yet to work with\nit efficiently, they need to learn to trust it. This is accomplished by\neducating the team working with the AI so they can understand how and why the\nAI makes decisions.\n\nExplainability versus interpretability in AI\n\nInterpretability is the degree to which an observer can understand the cause\nof a decision.", "start_char_idx": 5854, "end_char_idx": 7032, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d268e45d-6d58-4c89-9a44-3caf786a32ac": {"__data__": {"id_": "d268e45d-6d58-4c89-9a44-3caf786a32ac", "embedding": null, "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.ibm.com/topics/explainable-ai", "node_type": "4", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "d1ff78a3bd8dbb6f44766135e11120a63a9f3a736deae13896daf7917279ea0f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16566743-ce54-4b5c-a21e-7de497a8b149", "node_type": "1", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "bf4821efb135ddf1e98be9152f4f086bba72054f1ab2f9f92fcc9c64a4dda1dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53d12853-52b6-4f0d-8d91-f567570d714f", "node_type": "1", "metadata": {}, "hash": "a090f894c4a1699d678459a64dd8fd9598caf8e25a2d2f8c3c6014acc065c7b0", "class_name": "RelatedNodeInfo"}}, "text": "Explainability versus interpretability in AI\n\nInterpretability is the degree to which an observer can understand the cause\nof a decision. It is the success rate that humans can predict for the result\nof an AI output, while explainability goes a step further and looks at how the\nAI arrived at the result.\n\nHow does explainable AI relate to responsible AI?\n\nExplainable AI and responsible AI have similar objectives, yet different\napproaches. Here are the main differences between explainable and responsible\nAI:\n\n  * Explainable AI looks at AI results after the results are computed.\n  * Responsible AI looks at AI during the planning stages to make the AI algorithm responsible before the results are computed.\n  * Explainable and responsible AI can work together to make better AI.\n\nContinuous model evaluation\n\nWith explainable AI, a business can troubleshoot and improve model performance\nwhile helping stakeholders understand the behaviors of AI models.\nInvestigating model behaviors through tracking model insights on deployment\nstatus, fairness, quality and drift is essential to scaling AI.\n\nContinuous model evaluation empowers a business to compare model predictions,\nquantify model risk and optimize model performance. Displaying positive and\nnegative values in model behaviors with data used to generate explanation\nspeeds model evaluations.", "start_char_idx": 6895, "end_char_idx": 8248, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53d12853-52b6-4f0d-8d91-f567570d714f": {"__data__": {"id_": "53d12853-52b6-4f0d-8d91-f567570d714f", "embedding": null, "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.ibm.com/topics/explainable-ai", "node_type": "4", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "d1ff78a3bd8dbb6f44766135e11120a63a9f3a736deae13896daf7917279ea0f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d268e45d-6d58-4c89-9a44-3caf786a32ac", "node_type": "1", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "591cbb770a5a0345fc015375470fe359ed5c993f337fd9d0855c698efde2f4bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "106e0837-b60e-4d6e-b2ca-c28711221357", "node_type": "1", "metadata": {}, "hash": "ec3a9d7649c42866e6308bbdd0c563803a707b9c9644dfed6bb9436c2a221cba", "class_name": "RelatedNodeInfo"}}, "text": "Displaying positive and\nnegative values in model behaviors with data used to generate explanation\nspeeds model evaluations. A data and AI platform can generate feature\nattributions for model predictions and empower teams to visually investigate\nmodel behavior with interactive charts and exportable documents.\n\nBenefits of explainable AI\n\n__ Operationalize AI with trust and confidence\n\nBuild trust in production AI. Rapidly bring your AI models to production.\nEnsure interpretability and explainability of AI models. Simplify the process\nof model evaluation while increasing model transparency and traceability.\n\n__ Speed time to AI results\n\nSystematically monitor and manage models to optimize business outcomes.\nContinually evaluate and improve model performance. Fine-tune model\ndevelopment efforts based on continuous evaluation.\n\n__ Mitigate risk and cost of model governance\n\nKeep your AI models explainable and transparent. Manage regulatory,\ncompliance, risk and other requirements. Minimize overhead of manual\ninspection and costly errors. Mitigate risk of unintended bias.\n\nFive considerations for explainable AI\n\nTo drive desirable outcomes with explainable AI, consider the following.\n\nFairness and debiasing: Manage and monitor fairness. Scan your deployment for\npotential biases.\n\nModel drift mitigation: Analyze your model and make recommendations based on\nthe most logical outcome. Alert when models deviate from the intended\noutcomes.", "start_char_idx": 8125, "end_char_idx": 9577, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "106e0837-b60e-4d6e-b2ca-c28711221357": {"__data__": {"id_": "106e0837-b60e-4d6e-b2ca-c28711221357", "embedding": null, "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.ibm.com/topics/explainable-ai", "node_type": "4", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "d1ff78a3bd8dbb6f44766135e11120a63a9f3a736deae13896daf7917279ea0f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53d12853-52b6-4f0d-8d91-f567570d714f", "node_type": "1", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "a68fbb3c3f0605c3a5f335abbf514baeb27b7fbe5c8d69528eb023a1238d0225", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b21addf-c14e-4402-9d91-b0101720a55a", "node_type": "1", "metadata": {}, "hash": "5c5d0f973b8df88961daa9907a40e6eaa751ae2e606f19e3eefceb1de755c41f", "class_name": "RelatedNodeInfo"}}, "text": "Alert when models deviate from the intended\noutcomes.\n\nModel risk management: Quantify and mitigate model risk. Get alerted when a\nmodel performs inadequately. Understand what happened when deviations persist.\n\nLifecycle automation: Build, run and manage models as part of integrated data\nand AI services. Unify the tools and processes on a platform to monitor models\nand share outcomes. Explain the dependencies of machine learning models.\n\nMulticloud-ready: Deploy AI projects across hybrid clouds including public\nclouds, private clouds and on premises. Promote trust and confidence with\nexplainable AI.\n\nUse cases for explainable AI\n\n  * **Healthcare:** Accelerate diagnostics, image analysis, resource optimization and medical diagnosis. Improve transparency and traceability in decision-making for patient care. Streamline the pharmaceutical approval process with explainable AI.\n\n  * **Financial services:** Improve customer experiences with a transparent loan and credit approval process. Speed credit risk, wealth management and financial crime risk assessments. Accelerate resolution of potential complaints and issues. Increase confidence in pricing, product recommendations and investment services.\n\n  * **Criminal justice:** Optimize processes for prediction and risk assessment. Accelerate resolutions using explainable AI on DNA analysis, prison population analysis and crime forecasting. Detect potential biases in training data and algorithms.", "start_char_idx": 9524, "end_char_idx": 10984, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b21addf-c14e-4402-9d91-b0101720a55a": {"__data__": {"id_": "6b21addf-c14e-4402-9d91-b0101720a55a", "embedding": null, "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.ibm.com/topics/explainable-ai", "node_type": "4", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "d1ff78a3bd8dbb6f44766135e11120a63a9f3a736deae13896daf7917279ea0f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "106e0837-b60e-4d6e-b2ca-c28711221357", "node_type": "1", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "6fdd68d34655f7edae0e92a3b18b568845893f3781278516c4790f933d3b11f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a00dfea6-58cf-45a6-8ffa-169bf7b0de66", "node_type": "1", "metadata": {}, "hash": "bd5112251ec0036cf0de3163b1f6318c80ba420284ce2f80d820fc9d02d2a88e", "class_name": "RelatedNodeInfo"}}, "text": "Accelerate resolutions using explainable AI on DNA analysis, prison population analysis and crime forecasting. Detect potential biases in training data and algorithms.\n\nRelated solutions\n\nwatsonx.governance\n\nIBM\u00ae watsonx.governance\u2122 toolkit for AI governance allows you to direct,\nmanage and monitor your organization\u2019s AI activities, and employs software\nautomation to strengthen your ability to mitigate risk, manage regulatory\nrequirements and address ethical concerns for both generative AI and machine\nlearning models.\n\nLearn about watsonx.governance\n\nIBM Cloud Pak\u00ae for Data\n\nModernize to automate the AI lifecycle. Add governance and security to data\nand AI services almost anywhere.\n\nLearn more\n\nIBM Watson\u00ae Studio\n\nBuild and scale AI with trust and transparency. Build, run and manage AI\nmodels with constant monitoring for explainable AI.\n\nLearn more\n\nIBM Knowledge Catalog\n\nGovern data and AI models with an end-to-end data catalog backed by active\nmetadata and policy management.\n\nLearn more\n\nResources  Guide  The urgency of AI governance\n\nRead about a three-step approach to AI governance. Discover insights on how to\nbuild governance systems capable of monitoring ethical AI.\n\nRead the guide Tutorial  Prepare models for monitoring\n\nLearn how to set up and enable model monitors. Use a credit risk sample model\nto select deployment and set the data type for payload logging.", "start_char_idx": 10817, "end_char_idx": 12206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a00dfea6-58cf-45a6-8ffa-169bf7b0de66": {"__data__": {"id_": "a00dfea6-58cf-45a6-8ffa-169bf7b0de66", "embedding": null, "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.ibm.com/topics/explainable-ai", "node_type": "4", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "d1ff78a3bd8dbb6f44766135e11120a63a9f3a736deae13896daf7917279ea0f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b21addf-c14e-4402-9d91-b0101720a55a", "node_type": "1", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "8110f2739ee11bc7eeda3f9f3c788d9a567e3fa02bd8a223a3387d31fff6346a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5251c649-75c1-4432-a865-69c8bfb128df", "node_type": "1", "metadata": {}, "hash": "0259fe281c1555f7056e98c58a96ccdd93f6af7ea634d68f71b0eb6aaea9699b", "class_name": "RelatedNodeInfo"}}, "text": "Use a credit risk sample model\nto select deployment and set the data type for payload logging.\n\nBegin tutorial Analyst report  Explore the value of explainable AI\n\nForrester Consulting examines the projected return on investment for\nenterprises that deploy explainable AI and model monitoring.\n\nRead the study Case study  Scale AI with trust and transparency\n\nLufthansa improves the customer experience and airline efficiency with AI\nlifecycle automation and drift and bias mitigation.\n\nRead the case study\n\nTake the next step\n\nAccelerate responsible, transparent and explainable AI workflows across the\nlifecycle for both generative and machine learning models. Direct, manage, and\nmonitor your organization\u2019s AI activities to better manage growing AI\nregulations and detect and mitigate risk.  \n\nExplore watsonx.governance Book a live demo\n\n#####  Footnotes\n\n\u00b9 \u201d[Explainable AI](https://royalsociety.org/topics-\npolicy/projects/explainable-ai)\u201d (link resides outside ibm.com), The Royal\nSociety, 28 November 2019. ****\n\n\u00b2 \u201d[Explainable Artificial\nIntelligence](https://towardsdatascience.com/explainable-artificial-\nintelligence-14944563cc79)\u201d (link resides outside ibm.com), Jaime Zornoza, 15\nApril 2020.", "start_char_idx": 12112, "end_char_idx": 13319, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5251c649-75c1-4432-a865-69c8bfb128df": {"__data__": {"id_": "5251c649-75c1-4432-a865-69c8bfb128df", "embedding": null, "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://www.ibm.com/topics/explainable-ai", "node_type": "4", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "d1ff78a3bd8dbb6f44766135e11120a63a9f3a736deae13896daf7917279ea0f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a00dfea6-58cf-45a6-8ffa-169bf7b0de66", "node_type": "1", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "ce1379863d9717c3f2806ecc034c6e2ed12bf4a9d4397e72d7ad07a9468a3888", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8b4220e-93e9-4cdf-be93-61bf715c1bb2", "node_type": "1", "metadata": {}, "hash": "47dd3f319444c6f0454f0533c123e9942a47f1c65d5115c39634c616b6971259", "class_name": "RelatedNodeInfo"}}, "text": "****\n\n\u00b3 \u201d[Explainable Artificial Intelligence (XAI): Concepts, Taxonomies,\nOpportunities and Challenges toward Responsible\nAI](https://www.sciencedirect.com/science/article/abs/pii/S1566253519308103)\u201d\n(link resides outside ibm.com), ScienceDirect, June 2020.\n\n\u2074 \u201d[Understanding Explainable\nAI](https://www.forbes.com/sites/cognitiveworld/2019/07/23/understanding-\nexplainable-ai/?sh=1e04b2bd7c9e)\u201d (link resides outside ibm.com), Ron\nSchmelzer, Forbes contributor, 23 July 2019.\n\n\u2075 \u201d[Explainable Artificial Intelligence\n(XAI)](https://www.darpa.mil/program/explainable-artificial-intelligence)\u201d\n(link resides outside ibm.com), Dr. Matt Turek, The U.S. Defense Advanced\nResearch Projects Agency (DARPA). ****", "start_char_idx": 13320, "end_char_idx": 14027, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8b4220e-93e9-4cdf-be93-61bf715c1bb2": {"__data__": {"id_": "a8b4220e-93e9-4cdf-be93-61bf715c1bb2", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5251c649-75c1-4432-a865-69c8bfb128df", "node_type": "1", "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}, "hash": "919b4b93829953ed789fe0293b5d18a84ab16c86b1f3534d6891eef2efbf4a51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d1a0380-9a34-429f-ab14-a96cae64e46a", "node_type": "1", "metadata": {}, "hash": "b9bf784573e8de47ed94500d46ee084611bdb314389d2d0ca0a63e84d42475a0", "class_name": "RelatedNodeInfo"}}, "text": "search menu icon-carat-right cmu-wordmark\n\n  * \u2715\n\n  * [About](https://www.sei.cmu.edu/about/index.cfm)\n[Leadership](https://www.sei.cmu.edu/about/leadership/index.cfm)\n[Divisions](https://www.sei.cmu.edu/about/divisions/index.cfm) [Work with\nUs](https://www.sei.cmu.edu/about/work-with-us/index.cfm) [Collaboration with\nCMU](https://www.sei.cmu.edu/about/collaboration-with-cmu/index.cfm) [History\nof Innovation at the SEI](https://www.sei.cmu.edu/about/history-of-innovation-\nat-the-sei/index.cfm)\n\n  * [Our Work](https://www.sei.cmu.edu/our-work/index.cfm)\n[Agile](https://www.sei.cmu.edu/our-work/agile/index.cfm) [Artificial\nIntelligence Engineering](https://www.sei.cmu.edu/our-work/artificial-\nintelligence-engineering/index.cfm) [Cyber Workforce\nDevelopment](https://www.sei.cmu.", "start_char_idx": 0, "end_char_idx": 786, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d1a0380-9a34-429f-ab14-a96cae64e46a": {"__data__": {"id_": "7d1a0380-9a34-429f-ab14-a96cae64e46a", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8b4220e-93e9-4cdf-be93-61bf715c1bb2", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "fbcb03b3fcb7df6b22436e236abefad451ecb9a507af5ebd65d8c0d5e6ebb2d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b36500f2-bc6a-46dd-98fd-cf2ccb82fa0e", "node_type": "1", "metadata": {}, "hash": "8b98159b8de372f51beb3b2add0528a203c8e930e0597bf7671160665c2953c3", "class_name": "RelatedNodeInfo"}}, "text": "cfm) [Cyber Workforce\nDevelopment](https://www.sei.cmu.edu/our-work/cyber-workforce-\ndevelopment/index.cfm) [Cybersecurity Center\nDevelopment](https://www.sei.cmu.edu/our-work/cybersecurity-center-\ndevelopment/index.cfm) [Cybersecurity\nEngineering](https://www.sei.cmu.edu/our-work/cybersecurity-\nengineering/index.cfm) [DevSecOps](https://www.sei.cmu.edu/our-\nwork/devsecops/index.cfm) [Enterprise Risk and Resilience\nManagement](https://www.sei.cmu.edu/our-work/enterprise-risk-resilience-\nmanagement/index.cfm) [All Topics](https://www.sei.cmu.edu/our-work/all-\ntopics/index.cfm) [All Projects](https://www.sei.cmu.edu/our-\nwork/projects/index.cfm)\n\n  * [Publications](https://www.sei.cmu.edu/publications/index.cfm)\n[Annual Reviews](https://www.sei.cmu.edu/publications/annual-\nreviews/index.", "start_char_idx": 731, "end_char_idx": 1527, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b36500f2-bc6a-46dd-98fd-cf2ccb82fa0e": {"__data__": {"id_": "b36500f2-bc6a-46dd-98fd-cf2ccb82fa0e", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d1a0380-9a34-429f-ab14-a96cae64e46a", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "edaf3a4afef56df78da99a2b170cfc69340ca4ae8dd364b75929cebb428028d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da841b1d-3071-42f9-a797-296637986953", "node_type": "1", "metadata": {}, "hash": "7d734fc9bd9f6c71b3d135da1d47699faaf2238542563487c7a37da36367c85d", "class_name": "RelatedNodeInfo"}}, "text": "edu/publications/index.cfm)\n[Annual Reviews](https://www.sei.cmu.edu/publications/annual-\nreviews/index.cfm) [Blog](/blog/) [Digital Library](/library/) [Podcast\nSeries](/podcasts/) [Software and Tools](/software-tools/) [Technical\nPapers](/technical-papers/) [Vulnerability Notes\nDatabase](https://www.kb.cert.org/vuls) [Webcasts](/webcasts/)\n\n  * [News and Events](https://www.sei.cmu.edu/news-events/index.cfm)\n[News](/news/) [Events](https://www.sei.cmu.edu/news-events/events/index.cfm)\n[SEI Bulletin](/sei-bulletin/)\n\n  * [Education and Outreach](https://www.sei.cmu.edu/education-outreach/index.cfm)\n[Courses](https://www.sei.cmu.edu/education-outreach/courses/index.cfm)\n[Credentials](https://www.sei.cmu.edu/education-\noutreach/credentials/index.cfm) [Curricula](https://www.sei.cmu.edu/education-\noutreach/curricula/index.", "start_char_idx": 1423, "end_char_idx": 2255, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da841b1d-3071-42f9-a797-296637986953": {"__data__": {"id_": "da841b1d-3071-42f9-a797-296637986953", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b36500f2-bc6a-46dd-98fd-cf2ccb82fa0e", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "c4e6550df8503d1951bfaf6ff7bdb28eb757b60d16b1c821eacba7ffc6298d57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5671e77b-3659-4835-b9dc-d009dd7a2b6a", "node_type": "1", "metadata": {}, "hash": "7358220e6ee309d10b7cbf2d0e20639fdfdee57e22d1d37c86e573b4cc736607", "class_name": "RelatedNodeInfo"}}, "text": "cfm) [Curricula](https://www.sei.cmu.edu/education-\noutreach/curricula/index.cfm) [License SEI\nMaterials](https://www.sei.cmu.edu/education-outreach/license-sei-\nmaterials/index.cfm)\n\n  * [Careers](https://www.sei.cmu.edu/careers/index.cfm)\n[Job Openings](https://cmu.wd5.myworkdayjobs.com/SEI) [Diversity, Equity and\nInclusion](https://www.sei.cmu.edu/careers/diversity-equity-\ninclusion/index.cfm) [Internship\nOpportunities](https://www.sei.cmu.edu/careers/internship-\nopportunities/index.cfm) [Working at the\nSEI](https://www.sei.cmu.edu/careers/working-at-the-sei/index.cfm)\n\n[ Carnegie Mellon University ](https://www.cmu.edu)\n\n#  [ Software Engineering Institute ](https://www.sei.cmu.edu)\n\n  * [About](https://www.sei.cmu.edu/about/index.cfm)\n\n[Leadership](https://www.sei.cmu.", "start_char_idx": 2178, "end_char_idx": 2962, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5671e77b-3659-4835-b9dc-d009dd7a2b6a": {"__data__": {"id_": "5671e77b-3659-4835-b9dc-d009dd7a2b6a", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da841b1d-3071-42f9-a797-296637986953", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "affd8028be3f3aa1475e4c4c043de7e2ddbd9cec552f48989f51504158b36e40", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9df11f5c-a6cf-4635-a60e-084639ecdb3d", "node_type": "1", "metadata": {}, "hash": "786a73d727df32efd6c4ac20344997d4b7540fac8d31f626286786346fe19d73", "class_name": "RelatedNodeInfo"}}, "text": "sei.cmu.edu/about/index.cfm)\n\n[Leadership](https://www.sei.cmu.edu/about/leadership/index.cfm)\n[Divisions](https://www.sei.cmu.edu/about/divisions/index.cfm) [Work with\nUs](https://www.sei.cmu.edu/about/work-with-us/index.cfm) [Collaboration with\nCMU](https://www.sei.cmu.edu/about/collaboration-with-cmu/index.cfm) [History\nof Innovation at the SEI](https://www.sei.cmu.edu/about/history-of-innovation-\nat-the-sei/index.cfm)\n\n  * [Our Work](https://www.sei.cmu.edu/our-work/index.cfm)\n\n[Agile](https://www.sei.cmu.edu/our-work/agile/index.cfm) [Artificial\nIntelligence Engineering](https://www.sei.cmu.edu/our-work/artificial-\nintelligence-engineering/index.cfm) [Cloud\nComputing](https://www.sei.cmu.edu/our-work/cloud-computing/index.cfm) [Cyber\nWorkforce Development](https://www.sei.cmu.", "start_char_idx": 2899, "end_char_idx": 3691, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9df11f5c-a6cf-4635-a60e-084639ecdb3d": {"__data__": {"id_": "9df11f5c-a6cf-4635-a60e-084639ecdb3d", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5671e77b-3659-4835-b9dc-d009dd7a2b6a", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "b06c5fdaee6b469de0decc57a6928110f4371cc0e875e3afbf9dc84045a1a370", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89562f45-aab8-4a3c-9cfc-e2000596ed19", "node_type": "1", "metadata": {}, "hash": "4bba52a8ef66aabafab2112beeb8ecb3a8d28fd04ddb7767ceab2f0a83bfbd5c", "class_name": "RelatedNodeInfo"}}, "text": "edu/our-work/cloud-computing/index.cfm) [Cyber\nWorkforce Development](https://www.sei.cmu.edu/our-work/cyber-workforce-\ndevelopment/index.cfm) [Cybersecurity Center\nDevelopment](https://sei.cmu.edu/our-work/cybersecurity-center-\ndevelopment/index.cfm) [Cybersecurity Engineering](https://sei.cmu.edu/our-\nwork/cybersecurity-engineering/index.cfm)\n[DevSecOps](https://www.sei.cmu.edu/our-work/devsecops/index.cfm) [All\nTopics](https://www.sei.cmu.edu/our-work/all-topics/index.cfm) [All\nProjects](https://www.sei.cmu.edu/our-work/projects/index.cfm)\n\n  * [Publications](https://www.sei.cmu.edu/publications/index.cfm)\n\n[Annual Reviews](https://www.sei.cmu.edu/publications/annual-\nreviews/index.", "start_char_idx": 3601, "end_char_idx": 4295, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89562f45-aab8-4a3c-9cfc-e2000596ed19": {"__data__": {"id_": "89562f45-aab8-4a3c-9cfc-e2000596ed19", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9df11f5c-a6cf-4635-a60e-084639ecdb3d", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "1136e643b83c593805b33b71159ac61b23840d694d0fc00a2dbe4655cc13c246", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "039da86e-f098-4183-8bb1-fe348e2cf70c", "node_type": "1", "metadata": {}, "hash": "fa6b68c4b3fef1499f280b3555c3ca6193ccd6a0124435bb717cf7a7f56df7f1", "class_name": "RelatedNodeInfo"}}, "text": "edu/publications/index.cfm)\n\n[Annual Reviews](https://www.sei.cmu.edu/publications/annual-\nreviews/index.cfm) [Blog](/blog/) [Digital Library](/library/) [Podcast\nSeries](/podcasts/) [Software and Tools](/software-tools/) [Technical\nPapers](/technical-papers/) [Vulnerability Notes\nDatabase](https://www.kb.cert.org/vuls) [Webcasts Series](/webcasts/)\n\n  * [News and Events](https://www.sei.cmu.edu/news-events/index.cfm)\n\n[News](/news/) [Events](https://www.sei.cmu.edu/news-events/events/index.cfm)\n[SEI Bulletin](/sei-bulletin/)\n\n  * [Education and Outreach](https://www.sei.cmu.edu/education-outreach/index.cfm)\n\n[Courses](https://www.sei.cmu.edu/education-outreach/courses/index.cfm)\n[Credentials](/credentials/) [Curricula](https://www.sei.cmu.edu/education-\noutreach/curricula/index.cfm) [License SEI Materials](/license-sei-materials/)\n\n  * [Careers](https://www.sei.", "start_char_idx": 4190, "end_char_idx": 5065, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "039da86e-f098-4183-8bb1-fe348e2cf70c": {"__data__": {"id_": "039da86e-f098-4183-8bb1-fe348e2cf70c", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89562f45-aab8-4a3c-9cfc-e2000596ed19", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "dec7fadc95c13c449fefcb172a4ed839f8b2696e9f524f189d5b26fe1cd449f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8badb81f-4864-4363-9e34-a532cdef91be", "node_type": "1", "metadata": {}, "hash": "3c626d879bfb0f2b8c14ad2c22adf879154898d2024205fdb20d2d976e69f11e", "class_name": "RelatedNodeInfo"}}, "text": "cfm) [License SEI Materials](/license-sei-materials/)\n\n  * [Careers](https://www.sei.cmu.edu/careers/index.cfm)\n\n[Job Openings](https://cmu.wd5.myworkdayjobs.com/SEI) [Diversity, Equity, and\nInclusion](https://www.sei.cmu.edu/careers/diversity-equity-\ninclusion/index.cfm) [Internship\nOpportunities](https://www.sei.cmu.edu/careers/internship-\nopportunities/index.cfm)\n\n[\n\n# SEI Blog\n\n](/blog/)\n\n  1. [Home](https://www.sei.cmu.edu/)\n  2. [Publications](https://www.sei.cmu.edu/publications/)\n  3. [Blog](/blog/)\n  4. What is Explainable AI? \n\n### Cite This Post\n\n\u00d7\n\n  * AMS\n  * APA\n  * Chicago\n  * IEEE\n  * BibTeX\n\nAMS Citation\n\nTurri, V., 2022: What is Explainable AI?.", "start_char_idx": 4980, "end_char_idx": 5651, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8badb81f-4864-4363-9e34-a532cdef91be": {"__data__": {"id_": "8badb81f-4864-4363-9e34-a532cdef91be", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "039da86e-f098-4183-8bb1-fe348e2cf70c", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "dd4bc167e83877d13916a6d3db57928b123e2bc38cef1f0af8607335c6e3b63a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d267af47-ac77-4ef0-8738-a1e895a247ed", "node_type": "1", "metadata": {}, "hash": "3cd588b415c0f29bb585398a34c1543fd26aefa6d59cf0ec67a108029b16cd57", "class_name": "RelatedNodeInfo"}}, "text": "Carnegie Mellon University, Software\nEngineering Institute's Insights (blog), Accessed April 29, 2024,\nhttps://insights.sei.cmu.edu/blog/what-is-explainable-ai/.\n\nCopy\n\nAPA Citation\n\nTurri, V. (2022, January 17). What is Explainable AI?. Retrieved April 29,\n2024, from https://insights.sei.cmu.edu/blog/what-is-explainable-ai/.\n\nCopy\n\nChicago Citation\n\nTurri, Violet. \"What is Explainable AI?.\" _Carnegie Mellon University,\nSoftware Engineering Institute's Insights (blog)_. Carnegie Mellon's Software\nEngineering Institute, January 17, 2022.\nhttps://insights.sei.cmu.edu/blog/what-is-explainable-ai/.\n\nCopy\n\nIEEE Citation\n\nV. Turri, \"What is Explainable AI?,\" _Carnegie Mellon University, Software\nEngineering Institute's Insights (blog)_. Carnegie Mellon's Software\nEngineering Institute, 17-Jan-2022 [Online]. Available:\nhttps://insights.sei.cmu.edu/blog/what-is-explainable-ai/. [Accessed:\n29-Apr-2024].", "start_char_idx": 5652, "end_char_idx": 6559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d267af47-ac77-4ef0-8738-a1e895a247ed": {"__data__": {"id_": "d267af47-ac77-4ef0-8738-a1e895a247ed", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8badb81f-4864-4363-9e34-a532cdef91be", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "0e5ee061d2fe468071707f4685fd92082ffe7115bd05de227dae1e7fdb5c6f44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "716c5086-4dbd-4527-8071-2e9569c61009", "node_type": "1", "metadata": {}, "hash": "49f21f863a0d3dba4b85c72f88367e7e36e4b9cf6c2e5763ebfa316aa54983f8", "class_name": "RelatedNodeInfo"}}, "text": "[Accessed:\n29-Apr-2024].\n\nCopy\n\nBibTeX Code\n\n@misc{turri_2022,  \nauthor={Turri, Violet},  \ntitle={What is Explainable AI?},  \nmonth={Jan},  \nyear={2022},  \nhowpublished={Carnegie Mellon University, Software Engineering Institute's\nInsights (blog)},  \nurl={https://insights.sei.cmu.edu/blog/what-is-explainable-ai/},  \nnote={Accessed: 2024-Apr-29}  \n}\n\nCopy\n\n##  What is Explainable AI?\n\n![Headshot of Violet Turri](/media/images/thumb_big_v-\nturri_blog_authors_.max-180x180.format-webp.webp)\n\n######\n\n[Violet Turri](/authors/violet-turri)\n\n######  January 17, 2022\n\n##### PUBLISHED IN\n\n[Artificial Intelligence Engineering](/blog/topics/artificial-intelligence-\nengineering/)\n\n##### CITE\n\nGet Citation\n\n##### SHARE\n\n  * [ __](https://www.facebook.com/share.php?u=https%3A%2F%2Finsights.sei.cmu.", "start_char_idx": 6535, "end_char_idx": 7329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "716c5086-4dbd-4527-8071-2e9569c61009": {"__data__": {"id_": "716c5086-4dbd-4527-8071-2e9569c61009", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d267af47-ac77-4ef0-8738-a1e895a247ed", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "eef7d21e703ac3d63f593fd81ec6d7b74f1e804cd0240016e46ff756722a7ef0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53f9af88-1698-46ef-97f2-da2a48d83d65", "node_type": "1", "metadata": {}, "hash": "c56e04c0db997eb7391767be22d5a99c35ffb149ba31060e2daa7f427801c8f1", "class_name": "RelatedNodeInfo"}}, "text": "facebook.com/share.php?u=https%3A%2F%2Finsights.sei.cmu.edu%2Fblog%2Fwhat-is-explainable-ai%2F \"Share on Facebook\")\n  * [ __](https://twitter.com/intent/tweet?url=https%3A%2F%2Finsights.sei.cmu.edu%2Fblog%2Fwhat-is-explainable-ai%2F&text=What%20is%20Explainable%20AI%3F \"Share on X\")\n  * [ __](https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Finsights.sei.cmu.edu%2Fblog%2Fwhat-is-explainable-ai%2F \"Share on LinkedIn\")\n  * [ __](mailto:?subject=What%20is%20Explainable%20AI%3F&body=Check%20out%20this%20blog%20post%3A%0Ahttps%3A//insights.sei.cmu.edu/blog/what-is-explainable-ai/ \"Share via Email\")\n  * __\n\nThis post has been shared 13 times.", "start_char_idx": 7273, "end_char_idx": 7933, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53f9af88-1698-46ef-97f2-da2a48d83d65": {"__data__": {"id_": "53f9af88-1698-46ef-97f2-da2a48d83d65", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "716c5086-4dbd-4527-8071-2e9569c61009", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "801a6e929c93643e500c94a7c9a11da2aa8edd3c60bea0ffa9c7811a3525b6fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "adbc24d0-bcb8-4ed2-a1ea-c427579a3f4b", "node_type": "1", "metadata": {}, "hash": "2c1480197df5b6a3756e05c87ffa58162a6b9c5ef25aebea7190705b51ac0afd", "class_name": "RelatedNodeInfo"}}, "text": "edu/blog/what-is-explainable-ai/ \"Share via Email\")\n  * __\n\nThis post has been shared 13 times.\n\nConsider a production line in which workers run heavy, potentially dangerous\nequipment to manufacture steel tubing. Company executives hire a team of\nmachine learning (ML) practitioners to develop an artificial intelligence (AI)\nmodel that can assist the frontline workers in making safe decisions, with the\nhopes that this model will revolutionize their business by improving worker\nefficiency and safety. After an expensive development process, manufacturers\nunveil their complex, high-accuracy model to the production line expecting to\nsee their investment pay off. Instead, they see extremely limited adoption by\ntheir workers. What went wrong?\n\nThis hypothetical example, adapted from a real-world case study in McKinsey\u2019s\n[The State of AI in 2020](https://www.mckinsey.com/business-\nfunctions/mckinsey-analytics/our-insights/global-survey-the-state-of-ai-\nin-2020), demonstrates the crucial role that explainability plays in the world\nof AI. While the model in the example may have been safe and accurate, the\ntarget users did not **trust** the AI system because they didn\u2019t know how it\nmade decisions.", "start_char_idx": 7838, "end_char_idx": 9043, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "adbc24d0-bcb8-4ed2-a1ea-c427579a3f4b": {"__data__": {"id_": "adbc24d0-bcb8-4ed2-a1ea-c427579a3f4b", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53f9af88-1698-46ef-97f2-da2a48d83d65", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "ac1ceaa184dc1453028df7f6940f1efe86392ba259e529f82508dea1400292e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0a7836d-dde3-4642-a16b-debbcada7d2b", "node_type": "1", "metadata": {}, "hash": "7a39b78583f4ed4fa6d7c1f008880a9d1b623baadaec739ec7217fa2301b934f", "class_name": "RelatedNodeInfo"}}, "text": "End-users deserve to understand the underlying decision-making\nprocesses of the systems they are expected to employ, especially in high-\nstakes situations. Perhaps unsurprisingly, McKinsey found that improving the\nexplainability of systems led to increased technology adoption.\n\n[Explainable artificial intelligence\n(XAI)](https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.1391) is a\npowerful tool in answering critical _How?_ and _Why?_ questions about AI\nsystems and can be used to address rising ethical and legal concerns. As a\nresult, AI researchers have identified XAI as a [necessary feature of\ntrustworthy AI](https://arxiv.org/pdf/1710.00794.pdf), and explainability has\nexperienced a recent surge in attention. However, despite the growing interest\nin XAI research and the demand for explainability across disparate domains,\nXAI still suffers from a number of limitations. This blog post presents an\nintroduction to the current state of XAI, including the strengths and\nweaknesses of this practice.\n\n **The Basics of Explainable AI**\n\nDespite the prevalence of explainability research, exact definitions\nsurrounding explainable AI are not yet consolidated.", "start_char_idx": 9044, "end_char_idx": 10220, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0a7836d-dde3-4642-a16b-debbcada7d2b": {"__data__": {"id_": "b0a7836d-dde3-4642-a16b-debbcada7d2b", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "adbc24d0-bcb8-4ed2-a1ea-c427579a3f4b", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "97ccfcbbf00e7a816c021375d55c3ea75fbf8cb521cf7b0109fec65d4bfa42a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0dd3718f-1feb-4932-bbdc-a6199eede44f", "node_type": "1", "metadata": {}, "hash": "20b180a898a76a88d7365c3f7dc7f334734750c79b0b1ec51a96badd22d3ec1b", "class_name": "RelatedNodeInfo"}}, "text": "For the purposes of this\nblog post, [explainable AI](https://www.ibm.com/watson/explainable-ai) refers\nto the\n\n _set of processes and methods that allows human users to comprehend and trust\nthe results and output created by machine learning algorithms._\n\nThis definition captures a sense of the broad range of explanation types and\naudiences, and acknowledges that explainability techniques can be applied to a\nsystem, as opposed to always baked in.\n\nLeaders in academia, industry, and the government have been studying the\nbenefits of explainability and developing algorithms to address a wide range\nof contexts. In the healthcare domain, for instance, researchers have\nidentified explainability as a requirement for [AI clinical decision support\nsystems](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-01332-6)\nbecause the ability to interpret system outputs facilitates shared decision-\nmaking between medical professionals and patients and provides much-needed\nsystem transparency. In\n[finance](https://www.bnymellon.com/apac/en/insights/all-insights/why-every-\nfinancial-institution-should-consider-explainable-ai.html), explanations of AI\nsystems are used to meet regulatory requirements and equip analysts with the\ninformation needed to audit high-risk decisions.", "start_char_idx": 10221, "end_char_idx": 11519, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0dd3718f-1feb-4932-bbdc-a6199eede44f": {"__data__": {"id_": "0dd3718f-1feb-4932-bbdc-a6199eede44f", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0a7836d-dde3-4642-a16b-debbcada7d2b", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "c46dc99e99cbf8e8aa3adc75252f51faaed82c68a01af0552ab6dd248e47b3a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "569c0528-8d77-4eaa-932b-ff1adcc454ba", "node_type": "1", "metadata": {}, "hash": "fdf4f588520bed1f662246dbd257375f3e2029c5ab269a5ab22338dbece8a5be", "class_name": "RelatedNodeInfo"}}, "text": "Explanations can vary greatly in form based on context and intent. Figure 1\nbelow shows both human-language and heat-map explanations of model actions.\nThe ML model used below can detect hip fractures using frontal pelvic x-rays\nand is designed for use by doctors. The _Original report_ presents a \u201cground-\ntruth\u201d report from a doctor based on the x-ray on the far left. The _Generated\nreport_ consists of an explanation of the model\u2019s diagnosis and a heat-map\nshowing regions of the x-ray that impacted the decision. The _Generated\nreport_ provides doctors with an explanation of the model\u2019s diagnosis that can\nbe easily understood and vetted.\n\n[ ![Two pelvic x-rays comparing the original report and the generated\nreport.](/media/images/figure1_XAI_Turri_01172022.max-1280x720.format-\nwebp.webp) ](/media/images/figure1_XAI_Turri_01172022.original.png)\n\nFigure 1. A human-language explanation from \"Producing radiologist-quality\nreports for interpretable artificial intelligence.\"\n<https://arxiv.org/pdf/1806.00340.pdf>\n\nFigure 2 below depicts a highly technical, interactive visualization of the\nlayers of a neural network.", "start_char_idx": 11521, "end_char_idx": 12647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "569c0528-8d77-4eaa-932b-ff1adcc454ba": {"__data__": {"id_": "569c0528-8d77-4eaa-932b-ff1adcc454ba", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0dd3718f-1feb-4932-bbdc-a6199eede44f", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "233d97f22339a029ec3cf6a91b1cfd252ad6ed6eff40752d17d95bb3d4c218b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3be4d10-8f0e-429c-8132-c1317fc37533", "node_type": "1", "metadata": {}, "hash": "a55c1131e9d15ffc744ab440ee19c94c96e196906f97e55f2df800a63c1fdbd7", "class_name": "RelatedNodeInfo"}}, "text": "This open-source tool allows users to tinker with\nthe architecture of a neural network and watch how the individual neurons\nchange throughout training. Heat-map explanations of underlying ML model\nstructures can provide ML practitioners with important information about the\ninner workings of opaque models.\n\n[ ![Heat maps of neural network layers from TensorFlow\nPlayground.](/media/images/figure2_XAI_Turri_01172022.max-1280x720.format-\nwebp.webp) ](/media/images/figure2_XAI_Turri_01172022.original.png)\n\nFigure 2. Heat maps of neural network layers from TensorFlow Playground.\n\nFigure 3 below shows a graph produced by the [_What-If Tool_](https://pair-\ncode.github.io/what-if-tool/) depicting the relationship between two inference\nscore types. Through this interactive visualization, users can leverage\ngraphical explanations to analyze model performance across different \u201cslices\u201d\nof the data, determine which input attributes have the greatest impact on\nmodel decisions, and inspect their data for biases or outliers. These graphs,\nwhile most easily interpretable by ML experts, can lead to important insights\nrelated to performance and fairness that can then be communicated to non-\ntechnical stakeholders.\n\n[ ![Screenshot of a graph generated by Google\u2019s What-If\nTool.", "start_char_idx": 12648, "end_char_idx": 13924, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3be4d10-8f0e-429c-8132-c1317fc37533": {"__data__": {"id_": "a3be4d10-8f0e-429c-8132-c1317fc37533", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "569c0528-8d77-4eaa-932b-ff1adcc454ba", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "aad34bf32c966bec6b1264aa72910c3810574637b3247b94f41436cc618bd677", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39845546-ba5c-42aa-a3dc-e834b5cbfebf", "node_type": "1", "metadata": {}, "hash": "fbb9a00e8243094dd29d75d258f5162a189a391a4bec05b4bd0b29be5fcc1127", "class_name": "RelatedNodeInfo"}}, "text": "[ ![Screenshot of a graph generated by Google\u2019s What-If\nTool.](/media/images/figure3_XAI_Turri_01172022.max-1280x720.format-webp.webp)\n](/media/images/figure3_XAI_Turri_01172022.original.png)\n\nFigure 3. Graphs produced by Google\u2019s What-If Tool.\n\nExplainability aims to answer stakeholder questions about the decision-making\nprocesses of AI systems. Developers and ML practitioners can use explanations\nto ensure that ML model and AI system project requirements are met during\nbuilding, debugging, and testing. Explanations can be used to help non-\ntechnical audiences, such as end-users, gain a better understanding of how AI\nsystems work and clarify questions and concerns about their behavior. This\nincreased transparency helps build trust and supports system monitoring and\nauditability.\n\nTechniques for creating explainable AI have been developed and applied across\nall steps of the ML lifecycle.", "start_char_idx": 13863, "end_char_idx": 14763, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39845546-ba5c-42aa-a3dc-e834b5cbfebf": {"__data__": {"id_": "39845546-ba5c-42aa-a3dc-e834b5cbfebf", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3be4d10-8f0e-429c-8132-c1317fc37533", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "4f25ac7f2bed67a5804b0f34724cdcd08a0fcbfaefc0af74bd8212e14dbf6b29", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8aa20507-e00a-43f3-9866-f51390318b2d", "node_type": "1", "metadata": {}, "hash": "a14a1913e0d1082973d96c10828b3b7762043c1ece50c39feeac0c4b2c15516e", "class_name": "RelatedNodeInfo"}}, "text": "Techniques for creating explainable AI have been developed and applied across\nall steps of the ML lifecycle. Methods exist for analyzing the data used to\ndevelop models ([pre-modeling](https://towardsdatascience.com/the-how-of-\nexplainable-ai-pre-modelling-explainability-699150495fe4)), incorporating\ninterpretability into the architecture of a system ([explainable\nmodeling](https://medium.com/@bahador.khaleghi/the-how-of-explainable-ai-\nexplainable-modelling-55c8c43d7bed?sk=998bbb1d6d73722fd0d633a3cbc86b53)), and\nproducing post-hoc explanations of system behavior ([post-\nmodeling](https://towardsdatascience.com/the-how-of-explainable-ai-post-\nmodelling-explainability-8b4cbc7adf5f)).\n\n **Why Interest in XAI is Exploding**\n\nAs the field of AI has matured, increasingly complex [opaque\nmodels](https://towardsdatascience.com/black-box-and-white-box-models-towards-\nexplainable-ai-172d45bfc512) have been developed and deployed to solve hard\nproblems.", "start_char_idx": 14655, "end_char_idx": 15612, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8aa20507-e00a-43f3-9866-f51390318b2d": {"__data__": {"id_": "8aa20507-e00a-43f3-9866-f51390318b2d", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39845546-ba5c-42aa-a3dc-e834b5cbfebf", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "d40834cc928bf3727d568ce2d9c0a8803aa1ea02609c9a48cbded0880d1c3d65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "621495eb-3106-4a69-96d9-16c3196a2bcf", "node_type": "1", "metadata": {}, "hash": "76f096b725fb86d7074f655d5ccafb944819cb436f25516a9bf500c17289b73e", "class_name": "RelatedNodeInfo"}}, "text": "Unlike many predecessor models, these models, by the nature of their\narchitecture, are harder to understand and oversee. When such models fail or\ndo not behave as expected or hoped, it can be hard for developers and end-\nusers to pinpoint why or determine methods for addressing the problem. XAI\nmeets the emerging demands of [AI engineering](https://www.sei.cmu.edu/our-\nwork/artificial-intelligence-engineering/) by providing insight into the inner\nworkings of these opaque models. Oversight can result in significant\nperformance improvements. For example, a [study by\nIBM](https://www.ibm.com/watson/explainable-ai) suggests that users of their\nXAI platform achieved a 15 percent to 30 percent rise in model accuracy and a\n4.1 to 15.6 million dollar increase in profits.\n\nTransparency is also important given the current context of rising ethical\nconcerns surrounding AI. In particular, AI systems are becoming more prevalent\nin our lives, and their decisions can bear significant consequences.", "start_char_idx": 15613, "end_char_idx": 16610, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "621495eb-3106-4a69-96d9-16c3196a2bcf": {"__data__": {"id_": "621495eb-3106-4a69-96d9-16c3196a2bcf", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8aa20507-e00a-43f3-9866-f51390318b2d", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "6e963646aacc8aebadcc4b26027af687254c45aa6294f7ef8b34f1cd66c15b60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31a351dc-d940-422b-95b9-a5e852498d8e", "node_type": "1", "metadata": {}, "hash": "2f38ba2b042917b499ffe93785cb314fe942a1415a4438ddd5cd25ca1b649c45", "class_name": "RelatedNodeInfo"}}, "text": "In particular, AI systems are becoming more prevalent\nin our lives, and their decisions can bear significant consequences.\nTheoretically, these systems could help eliminate human bias from decision-\nmaking processes that are historically fraught with prejudice, such as\n[determining bail](https://www.wired.com/story/algorithms-supposed-fix-bail-\nsystem-they-havent/) or [assessing home loan\neligibility](https://www.forbes.com/sites/korihale/2021/09/02/ai-bias-\ncaused-80-of-black-mortgage-applicants-to-be-denied/?sh=758073de36fe). Despite\nefforts to remove racial discrimination from these processes through AI,\nimplemented systems unintentionally upheld discriminatory practices due to the\nbiased nature of the data on which they were trained. As reliance on AI\nsystems to make important real-world choices expands, it is paramount that\nthese systems are thoroughly vetted and developed using [responsible AI (RAI)\nprinciples](https://www.diu.mil/responsible-ai-guidelines).\n\nThe development of legal requirements to address ethical concerns and\nviolations is ongoing.", "start_char_idx": 16488, "end_char_idx": 17560, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31a351dc-d940-422b-95b9-a5e852498d8e": {"__data__": {"id_": "31a351dc-d940-422b-95b9-a5e852498d8e", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "621495eb-3106-4a69-96d9-16c3196a2bcf", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "2a2af55325e03b7eb93a74c8b6b7197fcc2f5381a3143147f966e3a6a16f1734", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12b34e20-4656-4930-8506-682c76a43aac", "node_type": "1", "metadata": {}, "hash": "9243be652a85b43f55e4a8b7fb4019d58e1c77070d8cc040546d5e782da20416", "class_name": "RelatedNodeInfo"}}, "text": "The development of legal requirements to address ethical concerns and\nviolations is ongoing. The European Union\u2019s 2016 [General Data Protection\nRegulation](https://eur-lex.europa.eu/legal-\ncontent/EN/TXT/?uri=CELEX%3A02016R0679-20160504) (GDPR), for instance, states\nthat when individuals are impacted by decisions made through \u201cautomated\nprocessing,\u201d they are entitled to \u201cmeaningful information about the logic\ninvolved.\u201d Likewise, the 2020 [California Consumer Privacy\nAct](https://oag.ca.gov/privacy/ccpa) (CCPA) dictates that users have a right\nto know inferences made about them by AI systems and what data was used to\nmake those inferences. As legal demand grows for transparency, researchers and\npractitioners push XAI forward to meet new stipulations.\n\n **Current Limitations of XAI**\n\nOne obstacle that XAI research faces is a lack of consensus on the definitions\nof several key terms. Precise definitions of explainable AI vary across papers\nand contexts. Some researchers use the terms _explainability_ and\n_interpretability_ interchangeably to refer to the concept of making models\nand their outputs understandable. Others draw a variety of distinctions\nbetween the terms.", "start_char_idx": 17468, "end_char_idx": 18653, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12b34e20-4656-4930-8506-682c76a43aac": {"__data__": {"id_": "12b34e20-4656-4930-8506-682c76a43aac", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31a351dc-d940-422b-95b9-a5e852498d8e", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "3600f0700d4a86a06eae06ac3d330b3fb773ba44b7d7599b33063fc4a356621d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18c8fc81-6253-4073-b831-415c6c5c4c62", "node_type": "1", "metadata": {}, "hash": "2ce013443106face1550f5d63c517a36920cc1d12917abb6d93eb61a3c4e42e4", "class_name": "RelatedNodeInfo"}}, "text": "Others draw a variety of distinctions\nbetween the terms. For instance, one [academic\nsource](https://www.nature.com/articles/s42256-019-0048-x) asserts that\n_explainability_ refers to _a priori_ explanations, while _interpretability_\nrefers to _a posterio_ explanations. Definitions within the domain of XAI must\nbe strengthened and clarified to provide a common language for describing and\nresearching XAI topics.\n\nIn a similar vein, while papers proposing new XAI techniques are abundant,\nreal-world guidance on how to select, implement, and test these explanations\nto support project needs is scarce. Explanations have been shown to improve\n_understanding_ of ML systems for many audiences, but their ability to build\n_trust_ among non-AI experts has been debated. Research is ongoing on how to\nbest leverage explainability to build trust among non-AI experts; [interactive\nexplanations](https://arxiv.org/pdf/2001.02478.pdf), including question-and-\nanswer based explanations, have shown promise.\n\nAnother subject of debate is the value of explainability compared to other\nmethods for providing transparency.", "start_char_idx": 18597, "end_char_idx": 19709, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18c8fc81-6253-4073-b831-415c6c5c4c62": {"__data__": {"id_": "18c8fc81-6253-4073-b831-415c6c5c4c62", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12b34e20-4656-4930-8506-682c76a43aac", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "61da7dc94042f5d33ac50d39e7c62903145f92449fd3662dad452c141c573482", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a4d654c-ec3b-446a-b6a0-8ca0b7fec326", "node_type": "1", "metadata": {}, "hash": "ecbaac375ee126a93bfa6e6ef7725832092ba3b41012c80d40fa9582b233b700", "class_name": "RelatedNodeInfo"}}, "text": "Another subject of debate is the value of explainability compared to other\nmethods for providing transparency. Although explainability for opaque models\nis in high demand, XAI practitioners run the risk of over-simplifying and/or\nmisrepresenting complicated systems. As a result, the argument has been made\nthat opaque models should be replaced altogether with [inherently\ninterpretable models](https://arxiv.org/pdf/1811.10154.pdf), in which\ntransparency is built in. Others argue that, particularly in the medical\ndomain, opaque models should be evaluated through [rigorous\ntesting](https://www-science-org.cmu.idm.oclc.org/doi/10.1126/science.abg1834)\nincluding clinical trials, rather than explainability. Human-centered XAI\nresearch contends that XAI needs to expand beyond technical transparency to\ninclude [social transparency](https://thegradient.pub/human-centered-\nexplainable-ai/).\n\n **Why is the SEI Exploring XAI?**\n\nExplainability has been identified by the [U.S.\ngovernment](https://crsreports.congress.gov/product/pdf/R/R46795) as a key\ntool for developing trust and transparency in AI systems.", "start_char_idx": 19599, "end_char_idx": 20709, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a4d654c-ec3b-446a-b6a0-8ca0b7fec326": {"__data__": {"id_": "1a4d654c-ec3b-446a-b6a0-8ca0b7fec326", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18c8fc81-6253-4073-b831-415c6c5c4c62", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "26391e6f1546932297be5b3bae5d781e89044859c2b2c0d5b47d184ba6d20110", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28c504b2-2097-4de0-9d83-5ce114f485d2", "node_type": "1", "metadata": {}, "hash": "8fe565d669490602a7d436c9da9772571472cd6ce5be78524acbdae314de682a", "class_name": "RelatedNodeInfo"}}, "text": "During her [opening\ntalk](https://www.defense.gov/News/News-Stories/Article/Article/2667212/hicks-\nannounces-new-artificial-intelligence-initiative/) at the Defense Department's\nArtificial Intelligence Symposium and Tech Exchange, Deputy Defense Secretary\nKathleen H. Hicks stated, \u201cOur operators must come to trust the outputs of AI\nsystems; our commanders must come to trust the legal, ethical and moral\nfoundations of explainable AI; and the American people must come to trust the\nvalues their DoD has integrated into every application.\u201d The DoD\u2019s efforts\ntowards developing what Hicks described as a \u201crobust responsible AI\necosystem,\u201d including the adoption of [ethical principles for\nAI](https://www.defense.gov/News/Releases/Release/Article/2091996/dod-adopts-\nethical-principles-for-artificial-intelligence/), indicate a rising demand for\nXAI within the government. Similarly, the U.S. Department of Health and Human\nServices lists an effort to \u201cpromote ethical, trustworthy AI use and\ndevelopment,\u201d including explainable AI, as one of the focus areas of their [AI\nstrategy](https://www.hhs.gov/sites/default/files/final-hhs-ai-strategy.pdf).", "start_char_idx": 20710, "end_char_idx": 21859, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28c504b2-2097-4de0-9d83-5ce114f485d2": {"__data__": {"id_": "28c504b2-2097-4de0-9d83-5ce114f485d2", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a4d654c-ec3b-446a-b6a0-8ca0b7fec326", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "6ef691a048d0df033701ffceada44bb33ef40c47192c35e47b370108669b3dc6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e003936c-f444-4aed-b2a0-dcb1690975d9", "node_type": "1", "metadata": {}, "hash": "f854cbd5fc82a3ea155c685264578e4ba19a39c4d1238f765b1f140b91202b82", "class_name": "RelatedNodeInfo"}}, "text": "To address stakeholder needs, the SEI is developing a growing body of XAI and\nresponsible AI work. In a month-long, exploratory project titled \u201cSurvey of\nthe State of the Art of Interactive XAI\u201d from May 2021, I collected and\nlabelled a corpus of 54 examples of open-source interactive AI tools from\nacademia and industry. Interactive XAI has been identified within the XAI\nresearch community as an important emerging area of research because\ninteractive explanations, unlike static, one-shot explanations, encourage user\nengagement and exploration. Findings from this survey will be published in a\nfuture blog post. Additional examples of the SEI\u2019s recent work in explainable\nand responsible AI are available below.\n\n##### Additional Resources\n\nSEI researchers Rotem Guttman and Carol Smith explored how explainability can\nbe used to answer end-users' questions in the context of game-play in their\npaper \u201c[Play for Real(ism)](https://dl.acm.org/doi/pdf/10.1145/3474655) \u2013\nUsing Games to Predict Human-AI interactions in the Real World\u201d, published\nalongside two CMU HCII researchers.", "start_char_idx": 21861, "end_char_idx": 22945, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e003936c-f444-4aed-b2a0-dcb1690975d9": {"__data__": {"id_": "e003936c-f444-4aed-b2a0-dcb1690975d9", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28c504b2-2097-4de0-9d83-5ce114f485d2", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "4c1e50d997f5da9cdd2d5edc627979a951e422e06e26af0738b891e9750a7278", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb9ab836-8cd3-429a-82cf-c28a2a860115", "node_type": "1", "metadata": {}, "hash": "bc319727d6a82fad38e89f1b9120dbb5ce95362b7bc752a52e76b43058ffef0f", "class_name": "RelatedNodeInfo"}}, "text": "Researchers Alex Van Deusen and Carol Smith contributed to the DIU\u2019s\n[Responsible AI Guidelines](https://www.defense.gov/News/News-\nStories/Article/Article/2847598/defense-innovation-unit-publishes-responsible-\nai-guidelines/) to provide practical guidance for implementing RAI.\n\n##### Written By\n\n[ ![Headshot of Violet Turri](/media/images/thumb_big_v-\nturri_blog_authors_.format-webp.max-180x180.webp) ](/authors/violet-turri/)\n\n### Violet Turri\n\n######  [Author Page](/authors/violet-turri/)\n\n######  [Digital Library\nPublications](https://resources.sei.cmu.edu/library/author.cfm?authorid=884143)\n\n######  [Send a Message](https://www.sei.cmu.edu/contact-\nus/index.cfm?f=Violet&l=Turri)\n\n##### More By The Author\n\n[\n\n#### Bridging the Gap between Requirements Engineering and Model Evaluation in\nMachine Learning\n\n](/blog/bridging-the-gap-between-requirements-engineering-and-model-\nevaluation-in-machine-learning/)\n\n######  December 15, 2022 \u2022 By [ Violet Turri](/authors/violet-turri/),", "start_char_idx": 22947, "end_char_idx": 23940, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb9ab836-8cd3-429a-82cf-c28a2a860115": {"__data__": {"id_": "cb9ab836-8cd3-429a-82cf-c28a2a860115", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e003936c-f444-4aed-b2a0-dcb1690975d9", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "d88b34a65b2c01cfb253708710c20dae739e97b9573b6beae570b5d91cd2cffe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e983d86-b462-4252-acb4-df7b4c093305", "node_type": "1", "metadata": {}, "hash": "5d05f0ac1af2a255d4f104cccc32fab9e80185e480bf9be62977c4ffbaa05010", "class_name": "RelatedNodeInfo"}}, "text": "2022 \u2022 By [ Violet Turri](/authors/violet-turri/), [ Eric\nHeim](/authors/eric-heim/)\n\n##### More In Artificial Intelligence Engineering\n\n[\n\n#### Applying Large Language Models to DoD Software Acquisition: An Initial\nExperiment\n\n](/blog/applying-large-language-models-to-dod-software-acquisition-an-initial-\nexperiment/)\n\n######  April 1, 2024 \u2022 By [ Douglas Schmidt (Vanderbilt\nUniversity)](/authors/douglas-schmidt/), [ John E. Robert](/authors/john-\nrobert/)\n\n[\n\n#### OpenAI Collaboration Yields 14 Recommendations for Evaluating LLMs for\nCybersecurity\n\n](/blog/openai-collaboration-yields-14-recommendations-for-evaluating-llms-\nfor-cybersecurity/)\n\n######  February 21, 2024 \u2022 By [ Jeff Gennari](/authors/jeffrey-gennari/), [\nShing-hon Lau](/authors/shing-hon-lau/), [ Samuel J. Perl](/authors/samuel-\nperl/)\n\n[\n\n#### Using ChatGPT to Analyze Your Code?", "start_char_idx": 23890, "end_char_idx": 24747, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e983d86-b462-4252-acb4-df7b4c093305": {"__data__": {"id_": "1e983d86-b462-4252-acb4-df7b4c093305", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb9ab836-8cd3-429a-82cf-c28a2a860115", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "55d1e3d7b373a99bf74b5a5a3473c0770863ecd05b8d42c3f3807d79bc87bad4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28b56442-dbe6-431d-a41f-efe33a34606c", "node_type": "1", "metadata": {}, "hash": "49f21f863a0d3dba4b85c72f88367e7e36e4b9cf6c2e5763ebfa316aa54983f8", "class_name": "RelatedNodeInfo"}}, "text": "[ Samuel J. Perl](/authors/samuel-\nperl/)\n\n[\n\n#### Using ChatGPT to Analyze Your Code? Not So Fast\n\n](/blog/using-chatgpt-to-analyze-your-code-not-so-fast/)\n\n######  February 12, 2024 \u2022 By [ Mark Sherman](/authors/mark-sherman/)\n\n[\n\n#### Creating a Large Language Model Application Using Gradio\n\n](/blog/creating-a-large-language-model-application-using-gradio/)\n\n######  December 4, 2023 \u2022 By [ Tyler Brooks](/authors/tyler-brooks/)\n\n[\n\n#### Generative AI Q&A: Applications in Software Engineering\n\n](/blog/generative-ai-question-and-answer-applications-in-software-\nengineering/)\n\n######  November 16, 2023 \u2022 By [ John E. Robert](/authors/john-robert/), [\nDouglas Schmidt (Vanderbilt University)](/authors/douglas-schmidt/)\n\n##### PUBLISHED IN\n\n[Artificial Intelligence Engineering](/blog/topics/artificial-intelligence-\nengineering/)\n\n##### CITE\n\nGet Citation\n\n##### SHARE\n\n  * [ __](https://www.facebook.com/share.php?u=https%3A%2F%2Finsights.sei.cmu.", "start_char_idx": 24661, "end_char_idx": 25616, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28b56442-dbe6-431d-a41f-efe33a34606c": {"__data__": {"id_": "28b56442-dbe6-431d-a41f-efe33a34606c", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e983d86-b462-4252-acb4-df7b4c093305", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "5a91f1e44192067a9a91d81f9eaf0710f024c519590ed862027590fda97b69ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61e29ec7-a1ae-407c-a199-2d4c8f45dd2c", "node_type": "1", "metadata": {}, "hash": "3444a1e22cd7ccd0f4aaa14915565db88d4be4df6ca8386d8f142f7de4e553c6", "class_name": "RelatedNodeInfo"}}, "text": "facebook.com/share.php?u=https%3A%2F%2Finsights.sei.cmu.edu%2Fblog%2Fwhat-is-explainable-ai%2F \"Share on Facebook\")\n  * [ __](https://twitter.com/intent/tweet?url=https%3A%2F%2Finsights.sei.cmu.edu%2Fblog%2Fwhat-is-explainable-ai%2F&text=What%20is%20Explainable%20AI%3F \"Share on X\")\n  * [ __](https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Finsights.sei.cmu.edu%2Fblog%2Fwhat-is-explainable-ai%2F \"Share on LinkedIn\")\n  * [ __](mailto:?subject=What%20is%20Explainable%20AI%3F&body=Check%20out%20this%20blog%20post%3A%0Ahttps%3A//insights.sei.cmu.edu/blog/what-is-explainable-ai/ \"Share via Email\")\n  * __\n\nThis post has been shared 13 times.", "start_char_idx": 7273, "end_char_idx": 7933, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61e29ec7-a1ae-407c-a199-2d4c8f45dd2c": {"__data__": {"id_": "61e29ec7-a1ae-407c-a199-2d4c8f45dd2c", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28b56442-dbe6-431d-a41f-efe33a34606c", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "801a6e929c93643e500c94a7c9a11da2aa8edd3c60bea0ffa9c7811a3525b6fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c50295b-6059-41ab-a501-534ccabbe808", "node_type": "1", "metadata": {}, "hash": "6b99062d0d04c482cf2bf37f5ef5ca9764b6d7a1cb01eae908f0b244f6c39569", "class_name": "RelatedNodeInfo"}}, "text": "edu/blog/what-is-explainable-ai/ \"Share via Email\")\n  * __\n\nThis post has been shared 13 times.\n\n### Get updates on our latest work.\n\nSign up to have the latest post sent to your inbox weekly.", "start_char_idx": 26125, "end_char_idx": 26317, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c50295b-6059-41ab-a501-534ccabbe808": {"__data__": {"id_": "9c50295b-6059-41ab-a501-534ccabbe808", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61e29ec7-a1ae-407c-a199-2d4c8f45dd2c", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "87ad5147af52c1093e8acdd4d8b3c719b92e360dccb93251616e623a32b87480", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "055aa26d-7ce4-4922-932a-6d2f9b897d65", "node_type": "1", "metadata": {}, "hash": "70d1f54b97ee660a139ad2a87b76a3c3a193dab6e2b8d8d5ee5718f8ccfeaa22", "class_name": "RelatedNodeInfo"}}, "text": "### Get updates on our latest work.\n\nSign up to have the latest post sent to your inbox weekly.\n\n[Subscribe](/blog/subscribe) [Get our RSS feed](/blog/feeds/latest/rss/)\n\n##### More In Artificial Intelligence Engineering\n\n[\n\n#### Applying Large Language Models to DoD Software Acquisition: An Initial\nExperiment\n\n](/blog/applying-large-language-models-to-dod-software-acquisition-an-initial-\nexperiment/)\n\n######  April 1, 2024 \u2022 By [ Douglas Schmidt (Vanderbilt\nUniversity)](/authors/douglas-schmidt/), [ John E. Robert](/authors/john-\nrobert/)\n\n[\n\n#### OpenAI Collaboration Yields 14 Recommendations for Evaluating LLMs for\nCybersecurity\n\n](/blog/openai-collaboration-yields-14-recommendations-for-evaluating-llms-\nfor-cybersecurity/)\n\n######  February 21, 2024 \u2022 By [ Jeff Gennari](/authors/jeffrey-gennari/), [\nShing-hon Lau](/authors/shing-hon-lau/), [ Samuel J. Perl](/authors/samuel-\nperl/)\n\n[\n\n#### Using ChatGPT to Analyze Your Code?", "start_char_idx": 26222, "end_char_idx": 27164, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "055aa26d-7ce4-4922-932a-6d2f9b897d65": {"__data__": {"id_": "055aa26d-7ce4-4922-932a-6d2f9b897d65", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c50295b-6059-41ab-a501-534ccabbe808", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "e1da1cbb16f4534b653f4a2a434014e4a4bb5e7a3b74f8eb806dd0876b36e86d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9148de6a-dc0b-42e7-ac6a-9c2d93eb401b", "node_type": "1", "metadata": {}, "hash": "7bf0957d18e6bd2339a9007757488445d39c5c6a757e4c8c5f4975621e3a9349", "class_name": "RelatedNodeInfo"}}, "text": "Not So Fast\n\n](/blog/using-chatgpt-to-analyze-your-code-not-so-fast/)\n\n######  February 12, 2024 \u2022 By [ Mark Sherman](/authors/mark-sherman/)\n\n[\n\n#### Creating a Large Language Model Application Using Gradio\n\n](/blog/creating-a-large-language-model-application-using-gradio/)\n\n######  December 4, 2023 \u2022 By [ Tyler Brooks](/authors/tyler-brooks/)\n\n[\n\n#### Generative AI Q&A: Applications in Software Engineering\n\n](/blog/generative-ai-question-and-answer-applications-in-software-\nengineering/)\n\n######  November 16, 2023 \u2022 By [ John E. Robert](/authors/john-robert/), [\nDouglas Schmidt (Vanderbilt University)](/authors/douglas-schmidt/)\n\n## Get updates on our latest work.\n\nEach week, our researchers write about the latest in software engineering,\ncybersecurity and artificial intelligence. Sign up to get the latest post sent\nto your inbox the day it's published.\n\n[Subscribe](/blog/subscribe) [Get our RSS feed](/blog/feeds/latest/rss/)\n\n  * [Report a Vulnerability to CERT/CC](https://www.kb.cert.", "start_char_idx": 27165, "end_char_idx": 28168, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9148de6a-dc0b-42e7-ac6a-9c2d93eb401b": {"__data__": {"id_": "9148de6a-dc0b-42e7-ac6a-9c2d93eb401b", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "055aa26d-7ce4-4922-932a-6d2f9b897d65", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "f9e97353df78ceb3ba92b1c1a84eb64e7d7c565eed77a9ce4a41566ab4aa3edd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f54cff7d-d34e-4ba6-ab1b-809a3eef9751", "node_type": "1", "metadata": {}, "hash": "3021074838e91d8a2801833f134a08ace2465dd1db7ecc6a35034cbf96cd747f", "class_name": "RelatedNodeInfo"}}, "text": "kb.cert.org/vuls/report/)\n  * [Subscribe to SEI Bulletin](https://www.sei.cmu.edu/subscribe-to-sei-bulletin/)\n  * [Request Permission to Use SEI Materials](https://www.sei.cmu.edu/legal/request-permission-to-use-sei-material)\n\nCarnegie Mellon University  \nSoftware Engineering Institute  \n4500 Fifth Avenue  \nPittsburgh, PA 15213-2612  \n[412-268-5800](tel:+14122685800)\n\n  * [ __](https://www.facebook.com/SEICMU/ \"Visit the SEI on Facebook\")\n  * [ __](https://twitter.com/SEInews \"Visit the SEI on X\")\n  * [ __](https://www.linkedin.com/company/software-engineering-institute \"Visit the SEI on LinkedIn\")\n  * [ __](https://www.youtube.com/user/TheSEICMU \"Visit the SEI on YouTube\")\n  * [ __](https://itunes.apple.com/us/podcast/software-engineering-institute-sei-podcast-series/id566573552?mt=2 \"Visit the SEI on Apple Podcasts\")\n\n[Contact Us](https://www.sei.cmu.", "start_char_idx": 28160, "end_char_idx": 29025, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f54cff7d-d34e-4ba6-ab1b-809a3eef9751": {"__data__": {"id_": "f54cff7d-d34e-4ba6-ab1b-809a3eef9751", "embedding": null, "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/", "node_type": "4", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "171c70e0c8fff86e2f8a85ddb987dcf8e8dce67c9a0be81d159ce8df31ad3b3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9148de6a-dc0b-42e7-ac6a-9c2d93eb401b", "node_type": "1", "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}, "hash": "0b4678ce4a8e5bfa9550e8c5da2fc0af36019a104ebeb5aa00a0c327a89c5830", "class_name": "RelatedNodeInfo"}}, "text": "sei.cmu.edu/contact-us/)\n\n  * [Office Locations](https://www.sei.cmu.edu/locations/index.cfm)\n  * [Additional Sites Directory](https://www.sei.cmu.edu/additional-sites-directory/index.cfm)\n  * [Legal](https://www.sei.cmu.edu/legal/index.cfm)\n  * [Privacy Notice](https://www.sei.cmu.edu/legal/privacy-notice/index.cfm)\n  * [CMU Ethics Hotline](https://www.cmu.edu/hr/resources/ethics-hotline.html)\n  * [www.sei.cmu.edu](https://www.sei.cmu.edu/index.cfm)\n\n2024\n\nCarnegie Mellon University", "start_char_idx": 29017, "end_char_idx": 29505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"https://www.ibm.com/topics/explainable-ai": {"node_ids": ["9f57d295-fafe-426e-822a-9cbba1f4a8f4", "1feb8645-6694-4722-af86-ba2e660a6849", "97a095d0-e0e0-46df-ba29-74126e965f18", "e1c7088a-a448-4e66-8926-a7baa3ad9c15", "e8d3c295-9075-462c-a203-d0a85d3fdc41", "16566743-ce54-4b5c-a21e-7de497a8b149", "d268e45d-6d58-4c89-9a44-3caf786a32ac", "53d12853-52b6-4f0d-8d91-f567570d714f", "106e0837-b60e-4d6e-b2ca-c28711221357", "6b21addf-c14e-4402-9d91-b0101720a55a", "a00dfea6-58cf-45a6-8ffa-169bf7b0de66", "5251c649-75c1-4432-a865-69c8bfb128df"], "metadata": {"doc_id": "https://www.ibm.com/topics/explainable-ai"}}, "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/": {"node_ids": ["a8b4220e-93e9-4cdf-be93-61bf715c1bb2", "7d1a0380-9a34-429f-ab14-a96cae64e46a", "b36500f2-bc6a-46dd-98fd-cf2ccb82fa0e", "da841b1d-3071-42f9-a797-296637986953", "5671e77b-3659-4835-b9dc-d009dd7a2b6a", "9df11f5c-a6cf-4635-a60e-084639ecdb3d", "89562f45-aab8-4a3c-9cfc-e2000596ed19", "039da86e-f098-4183-8bb1-fe348e2cf70c", "8badb81f-4864-4363-9e34-a532cdef91be", "d267af47-ac77-4ef0-8738-a1e895a247ed", "716c5086-4dbd-4527-8071-2e9569c61009", "53f9af88-1698-46ef-97f2-da2a48d83d65", "adbc24d0-bcb8-4ed2-a1ea-c427579a3f4b", "b0a7836d-dde3-4642-a16b-debbcada7d2b", "0dd3718f-1feb-4932-bbdc-a6199eede44f", "569c0528-8d77-4eaa-932b-ff1adcc454ba", "a3be4d10-8f0e-429c-8132-c1317fc37533", "39845546-ba5c-42aa-a3dc-e834b5cbfebf", "8aa20507-e00a-43f3-9866-f51390318b2d", "621495eb-3106-4a69-96d9-16c3196a2bcf", "31a351dc-d940-422b-95b9-a5e852498d8e", "12b34e20-4656-4930-8506-682c76a43aac", "18c8fc81-6253-4073-b831-415c6c5c4c62", "1a4d654c-ec3b-446a-b6a0-8ca0b7fec326", "28c504b2-2097-4de0-9d83-5ce114f485d2", "e003936c-f444-4aed-b2a0-dcb1690975d9", "cb9ab836-8cd3-429a-82cf-c28a2a860115", "1e983d86-b462-4252-acb4-df7b4c093305", "28b56442-dbe6-431d-a41f-efe33a34606c", "61e29ec7-a1ae-407c-a199-2d4c8f45dd2c", "9c50295b-6059-41ab-a501-534ccabbe808", "055aa26d-7ce4-4922-932a-6d2f9b897d65", "9148de6a-dc0b-42e7-ac6a-9c2d93eb401b", "f54cff7d-d34e-4ba6-ab1b-809a3eef9751"], "metadata": {"doc_id": "https://insights.sei.cmu.edu/blog/what-is-explainable-ai/"}}}}