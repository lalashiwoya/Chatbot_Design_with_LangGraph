{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You don't have any resources from the local pdfs regarding explainable_ai\n",
      "You don't have any resources from the youtube subtitles regarding explainable_ai\n"
     ]
    }
   ],
   "source": [
    "from utils import init_memory, init_llm\n",
    "from api.create_chain import init_chain\n",
    "from api.create_workflow import custome_workflow\n",
    "from dotenv import load_dotenv\n",
    "from api.utils import get_router_retriever\n",
    "from utils import read_configs_from_toml\n",
    "configs = read_configs_from_toml(\"config.toml\")\n",
    "retriever = get_router_retriever(configs)\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "graph = custome_workflow()\n",
    "llm = init_llm()\n",
    "chains = init_chain(llm)\n",
    "chat_history = init_memory(llm)\n",
    "\n",
    "question=\"how to finetune LLM\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = configs['topics']['topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_configs = read_configs_from_toml(\"tool_configs.toml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = {tool_configs[tool]['name']: tool_configs[tool][\"description\"] for tool in tool_configs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "dict_keys(['LLM-XAI Knowledge Expert', 'Heart Disease Expert', 'Off-Topic Expert'])\n"
     ]
    }
   ],
   "source": [
    "question = \"Hi\"\n",
    "input = {\"question\": question,\n",
    "         \"chat_history\": chat_history,\n",
    "         \"workers\": workers,\n",
    "         \"chains\": chains,\n",
    "         \"retriever\": retriever,\n",
    "         \"topics\": topics}\n",
    "\n",
    "events = graph.stream(\n",
    "    \n",
    "    input\n",
    ")\n",
    "for s in events:\n",
    "    if 'refine' in s:\n",
    "        result = s['refine']['generation']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "dict_keys(['LLM-XAI Knowledge Expert', 'Heart Disease Expert', 'Off-Topic Expert'])\n",
      "999\n",
      "{'event': 'on_chain_end', 'name': 'refine', 'run_id': 'a209d32a-d8cd-4a2c-b589-1f724fb3d09d', 'tags': ['graph:step:4'], 'metadata': {}, 'data': {'input': {'question': 'Hi', 'generation': ['Hello! How can I assist you today?', \"If the question is not related to the given topics, \\n                  respond with: 'Hi, that seems off-topic.'\\n\\n                 However, if the input is a greeting e.g. 'Hi' or 'Hello' and more, \\n                 the chatbot should respond politely with: 'Hi, what can I help you with today?\", 'The following response is generated by None.\\n\\nHello! How can I assist you today? If your question is related to Large Language Models (LLM) finetuning, Explainable AI, or Heart Disease, feel free to ask for more information on these topics. If not, please let me know how I can help you with something else.\\n\\nSources:\\n- None'], 'chat_history': ConversationSummaryBufferMemory(llm=ChatOpenAI(callbacks=[<langchain.callbacks.streaming_stdout_final_only.FinalStreamingStdOutCallbackHandler object at 0x000001D279FFE7D0>], client=<openai.resources.chat.completions.Completions object at 0x000001D273CC4310>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D27716E910>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', streaming=True), return_messages=True, max_token_limit=500), 'worker_trace': ['Off-Topic Expert'], 'workers': {'LLM-XAI Knowledge Expert': 'An Expert in answering questions about Large language models (LLMs) and Explainable AI (XAI).\\n', 'Heart Disease Expert': 'An expert in diagnosing heart disease.\\n', 'Off-Topic Expert': \"If you think the user's question is not related to the given\\ntopics, please use this expert.\\n\"}, 'retriever': <service.llama_index_retrive.RouterLlamaRetriever object at 0x000001D279FFA090>, 'chains': {'docter': ChatPromptTemplate(input_variables=['question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='\\nYou are a helpful doctor in diagnistion heart disease.\\n\\nAnswer this question from your patient.\\n\\nQuestion: {question}\\n\\n\\nAnswer:\\n\\n'))])\n",
      "| ChatOpenAI(callbacks=[<langchain.callbacks.streaming_stdout_final_only.FinalStreamingStdOutCallbackHandler object at 0x000001D279FFE7D0>], client=<openai.resources.chat.completions.Completions object at 0x000001D273CC4310>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D27716E910>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', streaming=True)\n",
      "| StrOutputParser(), 'init_answer': ChatPromptTemplate(input_variables=['chat_history', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['chat_history', 'question'], template='\\nYou are a helpful chatbot.\\n\\nAnswer this question from the user.\\n\\nQuestion: {question}\\n\\n\\nChat_History: {chat_history}\\n\\n\\nAnswer:\\n\\n'))])\n",
      "| ChatOpenAI(callbacks=[<langchain.callbacks.streaming_stdout_final_only.FinalStreamingStdOutCallbackHandler object at 0x000001D279FFE7D0>], client=<openai.resources.chat.completions.Completions object at 0x000001D273CC4310>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D27716E910>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', streaming=True)\n",
      "| StrOutputParser(), 'off_topic': ChatPromptTemplate(input_variables=['question', 'topics'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question', 'topics'], template=\"\\nTopics: {topics}\\n\\nIf the question is not related to the given topics, \\nrespond with: 'Hi, that seems off-topic.'\\n\\nHowever, if the input is a greeting e.g. 'Hi' or 'Hello' and more, \\nthe chatbot should respond politely with: 'Hi, what can I help you with today?\\n\\nQuestion : {question} \\n\\nAnswer: \\n\"))])\n",
      "| ChatOpenAI(callbacks=[<langchain.callbacks.streaming_stdout_final_only.FinalStreamingStdOutCallbackHandler object at 0x000001D279FFE7D0>], client=<openai.resources.chat.completions.Completions object at 0x000001D273CC4310>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D27716E910>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', streaming=True)\n",
      "| StrOutputParser(), 'refine': ChatPromptTemplate(input_variables=['answer', 'model_name', 'question', 'topics'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['answer', 'model_name', 'question', 'topics'], template='\\nYou are a helpful chatbot tasked with merging the previous answers to better \\nserve your user on given topics.\\n\\nInput:\\n\\nTopics: {topics}\\n\\nQuestion: {question}\\n\\nPrevious Answers: {answer}\\n\\n\\nGuidelines:\\n- Carefully consider whether the question is relevant to the given topics. \\n\\n- If the question or any of the previous answer indicates the question is irrelevant to the given topics, \\nyou should follow:\\n1. You final answer should not contain the answer to the question.\\n2. You should kinldy introduce our topics and guide the users to ask related questions.\\n3. At the start of the final answer, state:\\nThe following response is generated by {model_name}.\\n\\n4. In the previous answers, you should figure out what is content you should deliver, and \\nwhat are instructions.\\n5. Good formatting.\\n\\nOtherise:\\n\\n- If the question or any of the previous answer indicate the question is relevant \\nto the given topics, you should follow:\\n \\n1. While Merging answers, only using information from the given previous answers.\\n2. Final Answer should be detailed.\\n3. At the start of the final answer, state:\\nThe following response is generated by {model_name}.\\n\\n4. At the end of the final answer, if sources (urls or local file path) are given in \\nthe previous answer, include them in the end, and avoid duplicates in the sources.\\n5. Good formatting.\\n\\nAnswer:\\n\\n'))])\n",
      "| ChatOpenAI(callbacks=[<langchain.callbacks.streaming_stdout_final_only.FinalStreamingStdOutCallbackHandler object at 0x000001D279FFE7D0>], client=<openai.resources.chat.completions.Completions object at 0x000001D273CC4310>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D27716E910>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', streaming=True)\n",
      "| StrOutputParser(), 'retrieve': ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='\\nPlease respond to the question below using the information from the provided \"Context\".\\n\\nQuestion: {question} \\n\\n\\nContext: {context}\\n\\n\\nWhen writing your response, append a list of URLs or file path from the \"Context\" that were used as sources \\n\\nto derive your answer as the final part of the answer.\\n\\nAnswer:\\n\\n'))])\n",
      "| ChatOpenAI(callbacks=[<langchain.callbacks.streaming_stdout_final_only.FinalStreamingStdOutCallbackHandler object at 0x000001D279FFE7D0>], client=<openai.resources.chat.completions.Completions object at 0x000001D273CC4310>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D27716E910>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', streaming=True)\n",
      "| StrOutputParser(), 'supervisor': ChatPromptTemplate(input_variables=['initial_answer', 'question', 'topics', 'worker_names', 'workers'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['initial_answer', 'question', 'topics', 'worker_names', 'workers'], template='\\n\\n\\nYou are a helpful AI assistant, working in a team with other experts to address user inquiries. \\nYour primary role is to determine the most suitable expert to handle each question, \\nbased on their stated areas of expertise, or to advise stopping if no expert’s qualifications \\nappropriately match the question or its context.\\n\\nGiven the following inputs:\\nTopics: {topics}\\nQuestion: {question}\\nContext: {initial_answer}\\n\\nBelow is a list of available experts along with descriptions of their expertise:\\nExpert List: {workers}\\n\\nGuidelines for choosing the subsequent action:\\n\\n1. If the expert list is empty, respond with \"FINISH\".\\n2. Assess whether the question or initial answer relates to the provided topics. \\n3. Choose the expert whose expertise description most closely aligns with the question or context. \\nIf no expert’s expertise matches, respond with \"FINISH\".\\n\\nYour response should one of {worker_names} or FINISH\\n\\nNo other formats are allowed.\\n\\nAnswer:\\n\\n\\n'))])\n",
      "| ChatOpenAI(callbacks=[<langchain.callbacks.streaming_stdout_final_only.FinalStreamingStdOutCallbackHandler object at 0x000001D279FFE7D0>], client=<openai.resources.chat.completions.Completions object at 0x000001D273CC4310>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D27716E910>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', streaming=True)\n",
      "| StrOutputParser()}, 'topics': ['Large Language Models (LLM) finetuning', 'Explainable AI', 'Heart Disease'], 'model_name': None}, 'output': {'generation': ['Hello! How can I assist you today?', \"If the question is not related to the given topics, \\n                  respond with: 'Hi, that seems off-topic.'\\n\\n                 However, if the input is a greeting e.g. 'Hi' or 'Hello' and more, \\n                 the chatbot should respond politely with: 'Hi, what can I help you with today?\", 'The following response is generated by None.\\n\\nHello! How can I assist you today? If your question is related to Large Language Models (LLM) finetuning, Explainable AI, or Heart Disease, feel free to ask for more information on these topics. If not, please let me know how I can help you with something else.\\n\\nSources:\\n- None']}}}\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    async for event in graph.astream_events(\n",
    "        input,\n",
    "        version=\"v1\",\n",
    "        include_names=['refine'],\n",
    "        \n",
    "       ):\n",
    "        \n",
    "    #     content = chunk['messages'][0].content\n",
    "    #     marker = \"Final Answer:\"\n",
    "    #     if marker in content:\n",
    "            \n",
    "    #         response = content.split(marker)[-1].strip()\n",
    "    # memory.save_context({\"input\": message.content}, {\"output\": response})\n",
    "        if event['event'] == \"on_chain_end\":\n",
    "            print(\"999\")\n",
    "            print(event)\n",
    "            # print(event['data']['output']['generation'][-1])\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
